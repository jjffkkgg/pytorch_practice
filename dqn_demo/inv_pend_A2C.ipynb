{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReU7Cds5c_NS"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR1Z3B23eJZR"
   },
   "outputs": [],
   "source": [
    "# set env with setting (especially reward and max episode)\n",
    "gym.envs.register(\n",
    "    id='CartPole_prefer-v0',\n",
    "    entry_point='gym.envs.classic_control:CartPoleEnv',\n",
    "    max_episode_steps=700,      # CartPole-v0 uses 200\n",
    "    reward_threshold=-110.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjKI9y2ed3TF"
   },
   "outputs": [],
   "source": [
    "'''Global Variables'''\n",
    "ENV = 'CartPole_prefer-v0'  # 태스크 이름\n",
    "GAMMA = 0.99                # 시간할인율\n",
    "MAX_STEPS = 700             # 1에피소드 당 최대 단계 수\n",
    "NUM_EPISODES = 2000         # 최대 에피소드 수\n",
    "\n",
    "NUM_PROCESSES = 32          # 동시 실행 환경 수\n",
    "NUM_ADVANCED_STEP = 5       # 총 보상을 계산할 때 Advantage 학습(action actor)을 할 단계 수\n",
    "\n",
    "VALUE_LOSS_COEFF = 0.5\n",
    "ENTROPY_COEFF = 0.01        # Local min 에서 벗어나기 위한 엔트로피 상수\n",
    "MAX_GRAD_NORM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gx3uSUlDemU3"
   },
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    '''Advantage 학습에 사용할 메모리 클래스'''\n",
    "    def __init__(self, num_steps: int, num_processes: int, obs_shape: int) -> None:\n",
    "        '''initialize tensors for work'''\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, obs_shape)        # obs tensor init\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)                        # mask -> is end of episode ? 0 | 1\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
    "\n",
    "        # 할인 총 보상(J(theta,s_t))저장하는 메모리 (Actor)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.index = 0              # insert 하는 index          \n",
    "\n",
    "    def insert(self, current_obs: tensor, action: tensor, reward: tensor, mask: FloatTensor) -> None:\n",
    "        '''현재 인덱스 위치에 transition을 저장'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 값 업데이트\n",
    "\n",
    "    def after_update(self):\n",
    "        '''Advantage학습 단계만큼 단계가 진행되면 가장 새로운 transition을 index0에 저장'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value):\n",
    "        '''Advantage학습 범위 안의 각 단계에 대해 할인 총보상(J)을 계산'''\n",
    "\n",
    "        # 주의 : 5번째 단계부터 거슬러 올라오며 계산\n",
    "        # 주의 : 5번째 단계가 Advantage1, 4번째 단계는 Advantage2가 됨\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            # advantage 고려 Q함수의 수정\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_dMPRXZem3w"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in: int, n_mid: int, n_out: int) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in,n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid,n_mid)\n",
    "        self.actor = nn.Linear(n_mid, n_out)        # Actor net(two action output)\n",
    "        self.critic = nn.Linear(n_mid, 1)           # critic net (state value -> 1)\n",
    "    \n",
    "    def forward(self, x) -> list:\n",
    "        '''Forward wave 계산을 정의'''\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        critic_output = self.critic(h2)\n",
    "        actor_output = self.actor(h2)\n",
    "\n",
    "        return critic_output, actor_output\n",
    "    \n",
    "    def act(self, x) -> Tensor:\n",
    "        '''상태 x로부터 행동을 확률적으로 결정'''\n",
    "        value, actor_output = self(x)\n",
    "        action_probs = F.softmax(actor_output, dim=1)       # softmax 를 통한 행동의 확률 계산\n",
    "        action = action_probs.multinominal(num_samples=1)\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        '''상태 x로부터 상태가치를 계산'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zx1k8jyneoyx"
   },
   "outputs": [],
   "source": [
    "class Brain(object):\n",
    "    def __init__(self, actor_critic: Net) -> None:\n",
    "        self.actor_critic = actor_critic\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)    # learning rate = 0.01 (faster)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pl6ubIxseq-Y"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def run(self) -> None:\n",
    "        '''running entry point'''\n",
    "        # 동시 실행할 환경 수 만큼 env를 생성\n",
    "        envs = [gym.make(ENV) for i in range NUM_PROCESSES]\n",
    "\n",
    "        # 모든 에이전트가 공유하는 Brain 객체를 생성\n",
    "        n_in = envs[0].observation_space.shape[0]       # state inputs\n",
    "        n_out = envs.[0].action_space.n                 # action outpus\n",
    "        n_mid = 32                                      # 32 mid junction\n",
    "        actor_critic = Net(n_in, n_mid, n_out)          # Net init\n",
    "        glob_brain = Brain(actor_critic)                # Brain init\n",
    "\n",
    "        # 각종 정보를 저장하는 변수\n",
    "        obs_shape = n_in\n",
    "        current_obs = torch.zeros(NUM_PROCESSES, obs_shape)                         # (16,4) 의 tensor\n",
    "        rollouts = RolloutStorage(NUM_ADVANCED_STEP, NUM_PROCESSES,obs_shape)       # RolloutStorage init\n",
    "        episode_rewards = torch.zeros(NUM_PROCESSES, 1)                             # 현재 episode 의 reward\n",
    "        final_rewards = torch.zeros(NUM_PROCESSES, 1)                               # 마지막 episode 의 reward\n",
    "        obs_np = np.zeros([NUM_PROCESSES, obs_shape])                               # state 배열\n",
    "        reward_np = np.zeros([NUM_PROCESSES, 1])                                    # 보상의 배열\n",
    "        done_np = np.zeros([NUM_PROCESSES, 1])                                      # Done 여부의 배열\n",
    "        each_step = np.zeros(NUM_PROCESSES)                                         # 각 env 의 step record\n",
    "        episode = 0\n",
    "        # 초기 state...\n",
    "        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n",
    "        obs = np.array(obs)\n",
    "        obs = torch.from_numpy(obs).float()                     # (16,4) 의 tensor\n",
    "        current_obs = obs                                       # current obs 의 업데이트\n",
    "\n",
    "        # advanced 학습(action actor)에 사용되는 객체 rollouts 첫번째 상태에 현재 상태를 저장\n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "        # 에피소드 반복문\n",
    "        for episode in range(NUM_PROCESSES * NUM_EPISODES):\n",
    "            # advanced 학습(action actor) 대상이 되는 각 단계에 대해 계산 (step 반복)\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "                # action 을 fetch\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "                # (16,1)→(16,) -> tensor를 NumPy변수로\n",
    "                actions = action.squeeze(1).numpy()\n",
    "\n",
    "                # process 반복\n",
    "                for i in range(NUM_PROCESSES):\n",
    "                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(actions[i])\n",
    "\n",
    "                    # episode의 종료가치, state_next를 설정\n",
    "                    if done_np[i]:          # 지정된 step 달성 혹은 무너짐\n",
    "                        if i == 0:          # 0번째의 환경 결과만 출력\n",
    "                            print(f'{episode} Episode: Finished after {each_step[i] + 1} steps')\n",
    "                        episode += 1\n",
    "                        # 보상 부여\n",
    "                        if each_step[i] < (MAX_STEPS - 5):\n",
    "                            reward_np[i] = -1.0     # 무너졌을때\n",
    "                        else:\n",
    "                            reward_np[i] = 1.0      # 성공했을때\n",
    "                        \n",
    "                        each_step[i] = 0            # step 초기화\n",
    "                        obs_np[i] = envs[i].reset() # 환경 초기화\n",
    "                    else:                           # 무너지거나 성공도 아님\n",
    "                        reward_np[i] = 0.0\n",
    "                        each_step[i] += 1           # 그대로 진행\n",
    "\n",
    "                # 보상을 tensor로 변환하고, 에피소드의 총보상에 더해줌\n",
    "                reward = torch.from_numpy(reward_np).float()\n",
    "                episode_rewards += reward\n",
    "\n",
    "                # 각 실행 환경을 확인하여 done이 true이면 mask를 0으로, false이면 mask를 1로\n",
    "                masks = torch.FloatTensor([0.0] if done_i else [1.0] for done_i in done_np)\n",
    "\n",
    "                # 마지막 에피소드의 총 보상을 업데이트\n",
    "                final_rewards *= masks      # done이 false이면 1을 곱하고, true이면 0을 곱해 초기화\n",
    "                # done이 false이면 0을 더하고, true이면 episode_rewards를 더해줌\n",
    "                final_rewards += (1 - masks)*episode_rewards\n",
    "\n",
    "                # 에피소드의 총보상을 업데이트\n",
    "                episode_rewards *= masks\n",
    "\n",
    "                # 현재 done이 true이면 모두 0으로 \n",
    "                current_obs *= masks\n",
    "\n",
    "                # current_obs를 업데이트\n",
    "                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\n",
    "                current_obs = obs  # 최신 상태의 obs를 저장\n",
    "\n",
    "                # 메모리 객체에 현 단계의 transition을 저장\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "\n",
    "            # advanced 학습 for문 끝\n",
    "\n",
    "            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(rollouts.observations[-1]).detach()\n",
    "                # next_value는 다음 state의 상태가치\n",
    "                # rollouts.observations의 크기는 torch.Size([6, 16, 4])\n",
    "            \n",
    "            # 모든 단계의 할인총보상을 계산하고, rollouts의 변수 returns를 업데이트\n",
    "            rollouts.compute_returns(next_value)\n",
    "\n",
    "            # 신경망 및 rollout 업데이트\n",
    "            rollouts.after_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0nn2cr_exWL"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    cartpole_env = Environment()\n",
    "    cartpole_env.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOr0ByQEpzG3R2YVyMRCKbO",
   "collapsed_sections": [],
   "name": "inv_pend_A2C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('pyt': conda)",
   "language": "python",
   "name": "python37864bitpytcondafd1b43a097a64d34bd7b82617eda6abc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}