{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"inv_pend_A2C.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3.7.8 64-bit ('pyt': conda)","language":"python","name":"python37864bitpytcondafd1b43a097a64d34bd7b82617eda6abc"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"ReU7Cds5c_NS","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"error","timestamp":1599567495461,"user_tz":-540,"elapsed":1130,"user":{"displayName":"Taehwan Seo","photoUrl":"","userId":"06395709366113538068"}},"outputId":"e28e84e1-0315-4938-c9ca-3ab754e0bcda"},"source":["from __future__ import annotations\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import gym\n","import torch\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy"],"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7462c30f1ca6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from __future__ import annotations\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m future feature annotations is not defined\n"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vR1Z3B23eJZR","colab":{}},"source":["# set env with setting (especially reward and max episode)\n","gym.envs.register(\n","    id='CartPole_prefer-v0',\n","    entry_point='gym.envs.classic_control:CartPoleEnv',\n","    max_episode_steps=700,      # CartPole-v0 uses 200\n","    reward_threshold=-110.0,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YjKI9y2ed3TF","colab":{}},"source":["'''Global Variables'''\n","ENV = 'CartPole_prefer-v0'  # 태스크 이름\n","GAMMA = 0.99                # 시간할인율\n","MAX_STEPS = 700             # 1에피소드 당 최대 단계 수\n","NUM_EPISODES = 2000         # 최대 에피소드 수\n","\n","NUM_PROCESSES = 32          # 동시 실행 환경 수\n","NUM_ADVANCED_STEP = 5       # 총 보상을 계산할 때 Advantage 학습(action actor)을 할 단계 수\n","\n","VALUE_LOSS_COEFF = 0.5\n","ENTROPY_COEFF = 0.01        # Local min 에서 벗어나기 위한 엔트로피 상수\n","MAX_GRAD_NORM = 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gx3uSUlDemU3","colab":{}},"source":["class RolloutStorage(object):\n","    '''Advantage 학습에 사용할 메모리 클래스'''\n","    def __init__(self, num_steps: int, num_processes: int, obs_shape: int) -> None:\n","        '''initialize tensors for work'''\n","        self.observations = torch.zeros(num_steps + 1, num_processes, obs_shape)        # obs tensor init\n","        self.masks = torch.ones(num_steps + 1, num_processes, 1)                        # mask -> is end of episode ? 0 | 1\n","        self.rewards = torch.zeros(num_steps, num_processes, 1)\n","        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n","\n","        # 할인 총 보상(J(theta,s_t))저장하는 메모리 (Actor)\n","        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n","        self.index = 0              # insert 하는 index          \n","\n","    def insert(self, current_obs: tensor, action: tensor, reward: tensor, mask: FloatTensor) -> None:\n","        '''현재 인덱스 위치에 transition을 저장'''\n","        self.observations[self.index + 1].copy_(current_obs)\n","        self.masks[self.index + 1].copy_(mask)\n","        self.rewards[self.index].copy_(reward)\n","        self.actions[self.index].copy_(action)\n","\n","        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 값 업데이트\n","\n","    def after_update(self):\n","        '''Advantage학습 단계만큼 단계가 진행되면 가장 새로운 transition을 index0에 저장'''\n","        self.observations[0].copy_(self.observations[-1])\n","        self.masks[0].copy_(self.masks[-1])\n","\n","    def compute_returns(self, next_value):\n","        '''Advantage학습 범위 안의 각 단계에 대해 할인 총보상(J)을 계산'''\n","\n","        # 주의 : 5번째 단계부터 거슬러 올라오며 계산\n","        # 주의 : 5번째 단계가 Advantage1, 4번째 단계는 Advantage2가 됨\n","        self.returns[-1] = next_value\n","        for ad_step in reversed(range(self.rewards.size(0))):\n","            # advantage 고려 Q함수의 수정\n","            self.returns[ad_step] = self.returns[ad_step + 1] * GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J_dMPRXZem3w","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1599567608082,"user_tz":-540,"elapsed":1928,"user":{"displayName":"Taehwan Seo","photoUrl":"","userId":"06395709366113538068"}},"outputId":"866bb726-43ca-4217-b65d-5c121a52aca5"},"source":["class Net(nn.Module):\n","    def __init__(self, n_in: int, n_mid: int, n_out: int) -> None:\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(n_in,n_mid)\n","        self.fc2 = nn.Linear(n_mid,n_mid)\n","        self.actor = nn.Linear(n_mid, n_out)        # Actor net(two action output)\n","        self.critic = nn.Linear(n_mid, 1)           # critic net (state value -> 1)\n","    \n","    def forward(self, x) -> list:\n","        '''Forward wave 계산을 정의'''\n","        h1 = F.relu(self.fc1(x))\n","        h2 = F.relu(self.fc2(h1))\n","        critic_output = self.critic(h2)\n","        actor_output = self.actor(h2)\n","\n","        return critic_output, actor_output\n","    \n","    def act(self, x) -> Tensor:\n","        '''상태 x로부터 행동을 확률적으로 결정'''\n","        value, actor_output = self(x)\n","        action_probs = F.softmax(actor_output, dim=1)       # softmax 를 통한 행동의 확률 계산\n","        action = action_probs.multinominal(num_samples=1)\n","        return action\n","\n","    def get_value(self, x):\n","        '''상태 x로부터 상태가치를 계산'''\n","        value, actor_output = self(x)\n","\n","        return value\n","    \n","    def evaluate_actions(self, x: tensor, actions) -> list:\n","        '''상태 x로부터 상태가치, 실제 행동 actions의 로그 확률, 엔트로피를 계산'''\n","        value, actor_output = self(x)\n","\n","        log_probs = F.log_softmax(actor_output, dim=1)      # dim=1이므로 행동의 종류에 대해 확률을 계산\n","        action_log_probs = logs.probs.gather(1, actions)    # 실제 행동의 로그 확률(log_probs)을 구함\n","\n","        probs = F.softmax(actor_output, dim=1)\n","        entropy = -(log_probs * probs).sum(-1).mean()\n","\n","        return value, action_log_probs, entropy"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-bad17def5978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zx1k8jyneoyx","colab":{}},"source":["class Brain(object):\n","    def __init__(self, actor_critic: Net) -> None:\n","        self.actor_critic = actor_critic\n","        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)    # learning rate = 0.01 (faster)\n","        \n","    def update(self, rollouts: RolloutStorage) -> None:\n","        ''''Advantage학습의 대상이 되는 5단계 모두를 사용하여 수정'''\n","        obs_shape = rollouts.observations.size()[2:]    # torch.Size([4, 84, 84])\n","        num_steps = NUM_ADVANCED_STEP\n","        num_processes = NUM_PROCESSES\n","\n","        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n","            rollouts.observations[:-1].view(-1,4),\n","            rollouts.actions.view(-1,1)\n","        )\n","\n","        # 주의 : 각 변수의 크기\n","        # rollouts.observations[:-1].view(-1, 4) torch.Size([80, 4])\n","        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n","        # values torch.Size([80, 1])\n","        # action_log_probs torch.Size([80, 1])\n","        # entropy torch.Size([])\n","\n","        values = values.view(num_steps, num_processes, 1)   # torch.Size([5, 16, 1])\n","        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n","\n","        # advantage(행동가치-상태가치) 계산\n","        advatages = rollouts.returns[:-1] - values          # torch.Size([5, 16, 1])\n","\n","        # Critic loss 계산\n","        value_loss = advantages.pow(2).mean()\n","\n","        # Actor의 gain 계산, 나중에 -1을 곱하면 loss가 된다\n","        action_gain = (action_log_probs * advantages.detach()).mean()\n","        # detach 메서드를 호출하여 advantages를 상수로 취급\n","\n","        # 오차함수의 총합\n","        total_loss = (value_loss * value_loss_coef - action_gain - entropy * entropy_coef)\n","        \n","        # 가중치 수정\n","        self.actor_critic.train()\n","        self.optimizer.zero_grad()\n","        total_loss.backward()\n","        # 결합 가중치가 한번에 너무 크게 변화하지 않도록, 경사를 0.5 이하로 제한함(클리핑)\n","        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n","\n","        self.optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Pl6ubIxseq-Y","colab":{}},"source":["class Environment:\n","    def run(self) -> None:\n","        '''running entry point'''\n","        # 동시 실행할 환경 수 만큼 env를 생성\n","        envs = [gym.make(ENV) for i in range NUM_PROCESSES]\n","\n","        # 모든 에이전트가 공유하는 Brain 객체를 생성\n","        n_in = envs[0].observation_space.shape[0]       # state inputs\n","        n_out = envs.[0].action_space.n                 # action outpus\n","        n_mid = 32                                      # 32 mid junction\n","        actor_critic = Net(n_in, n_mid, n_out)          # Net init\n","        glob_brain = Brain(actor_critic)                # Brain init\n","\n","        # 각종 정보를 저장하는 변수\n","        obs_shape = n_in\n","        current_obs = torch.zeros(NUM_PROCESSES, obs_shape)                         # (16,4) 의 tensor\n","        rollouts = RolloutStorage(NUM_ADVANCED_STEP, NUM_PROCESSES,obs_shape)       # RolloutStorage init\n","        episode_rewards = torch.zeros(NUM_PROCESSES, 1)                             # 현재 episode 의 reward\n","        final_rewards = torch.zeros(NUM_PROCESSES, 1)                               # 마지막 episode 의 reward\n","        obs_np = np.zeros([NUM_PROCESSES, obs_shape])                               # state 배열\n","        reward_np = np.zeros([NUM_PROCESSES, 1])                                    # 보상의 배열\n","        done_np = np.zeros([NUM_PROCESSES, 1])                                      # Done 여부의 배열\n","        each_step = np.zeros(NUM_PROCESSES)                                         # 각 env 의 step record\n","        episode = 0\n","        # 초기 state...\n","        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n","        obs = np.array(obs)\n","        obs = torch.from_numpy(obs).float()                     # (16,4) 의 tensor\n","        current_obs = obs                                       # current obs 의 업데이트\n","\n","        # advanced 학습(action actor)에 사용되는 객체 rollouts 첫번째 상태에 현재 상태를 저장\n","        rollouts.observations[0].copy_(current_obs)\n","\n","        # 에피소드 반복문\n","        for episode in range(NUM_PROCESSES * NUM_EPISODES):\n","            # advanced 학습(action actor) 대상이 되는 각 단계에 대해 계산 (step 반복)\n","            for step in range(NUM_ADVANCED_STEP):\n","                # action 을 fetch\n","                with torch.no_grad():\n","                    action = actor_critic.act(rollouts.observations[step])\n","                # (16,1)→(16,) -> tensor를 NumPy변수로\n","                actions = action.squeeze(1).numpy()\n","\n","                # process 반복\n","                for i in range(NUM_PROCESSES):\n","                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(actions[i])\n","\n","                    # episode의 종료가치, state_next를 설정\n","                    if done_np[i]:          # 지정된 step 달성 혹은 무너짐\n","                        if i == 0:          # 0번째의 환경 결과만 출력\n","                            print(f'{episode} Episode: Finished after {each_step[i] + 1} steps')\n","                        episode += 1\n","                        # 보상 부여\n","                        if each_step[i] < (MAX_STEPS - 5):\n","                            reward_np[i] = -1.0     # 무너졌을때\n","                        else:\n","                            reward_np[i] = 1.0      # 성공했을때\n","                        \n","                        each_step[i] = 0            # step 초기화\n","                        obs_np[i] = envs[i].reset() # 환경 초기화\n","                    else:                           # 무너지거나 성공도 아님\n","                        reward_np[i] = 0.0\n","                        each_step[i] += 1           # 그대로 진행\n","\n","                # 보상을 tensor로 변환하고, 에피소드의 총보상에 더해줌\n","                reward = torch.from_numpy(reward_np).float()\n","                episode_rewards += reward\n","\n","                # 각 실행 환경을 확인하여 done이 true이면 mask를 0으로, false이면 mask를 1로\n","                masks = torch.FloatTensor([0.0] if done_i else [1.0] for done_i in done_np)\n","\n","                # 마지막 에피소드의 총 보상을 업데이트\n","                final_rewards *= masks      # done이 false이면 1을 곱하고, true이면 0을 곱해 초기화\n","                # done이 false이면 0을 더하고, true이면 episode_rewards를 더해줌\n","                final_rewards += (1 - masks)*episode_rewards\n","\n","                # 에피소드의 총보상을 업데이트\n","                episode_rewards *= masks\n","\n","                # 현재 done이 true이면 모두 0으로 \n","                current_obs *= masks\n","\n","                # current_obs를 업데이트\n","                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\n","                current_obs = obs  # 최신 상태의 obs를 저장\n","\n","                # 메모리 객체에 현 단계의 transition을 저장\n","                rollouts.insert(current_obs, action.data, reward, masks)\n","\n","            # advanced 학습 for문 끝\n","\n","            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n","\n","            with torch.no_grad():\n","                next_value = actor_critic.get_value(rollouts.observations[-1]).detach()\n","                # next_value는 다음 state의 상태가치\n","                # rollouts.observations의 크기는 torch.Size([6, 16, 4])\n","            \n","            # 모든 단계의 할인총보상을 계산하고, rollouts의 변수 returns를 업데이트\n","            rollouts.compute_returns(next_value)\n","\n","            # 신경망 및 rollout 업데이트\n","            glob_brain.update(rollouts)\n","            rollouts.after_update()\n","\n","            # 모든 환경이 성공\n","            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n","                print('모든 환경 성공')\n","                break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o0nn2cr_exWL","colab":{}},"source":["if __name__ == '__main__':\n","    cartpole_env = Environment()\n","    cartpole_env.run()"],"execution_count":null,"outputs":[]}]}