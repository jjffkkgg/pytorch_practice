{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.8 64-bit ('pyt': conda)",
      "language": "python",
      "name": "python37864bitpytcondafd1b43a097a64d34bd7b82617eda6abc"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8-final"
    },
    "colab": {
      "name": "inv_pend_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtD3NvHL38-M",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb56f8db-322b-4fbf-d3a5-03c8267c9688"
      },
      "source": [
        "'''\n",
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "random_episodes = 0\n",
        "reward_sum = 0\n",
        "while random_episodes < 10:\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    print(observation, reward, done)\n",
        "    reward_sum += reward\n",
        "    if done:\n",
        "        random_episodes += 1\n",
        "        print(f\"Reward for this episode was: {reward_sum}\")\n",
        "        reward_sum = 0\n",
        "        env.reset()\n",
        "env.close()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nenv = gym.make(\\'CartPole-v0\\')\\nenv.reset()\\nrandom_episodes = 0\\nreward_sum = 0\\nwhile random_episodes < 10:\\n    env.render()\\n    action = env.action_space.sample()\\n    observation, reward, done, _ = env.step(action)\\n    print(observation, reward, done)\\n    reward_sum += reward\\n    if done:\\n        random_episodes += 1\\n        print(f\"Reward for this episode was: {reward_sum}\")\\n        reward_sum = 0\\n        env.reset()\\nenv.close()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyySpKC_38-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import annotations\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import gym\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqR5U0bH38-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple(\n",
        "    'Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# set env with setting (especially reward and max episode)\n",
        "gym.envs.register(\n",
        "    id='CartPole_prefer-v0',\n",
        "    entry_point='gym.envs.classic_control:CartPoleEnv',\n",
        "    max_episode_steps=700,      # CartPole-v0 uses 200\n",
        "    reward_threshold=-110.0,\n",
        ")\n",
        "\n",
        "# 상수 정의\n",
        "ENV = 'CartPole_prefer-v0'     # 태스크 이름\n",
        "GAMMA = 0.99            # 시간할인율\n",
        "MAX_STEPS = 700         # 1에피소드 당 최대 단계 수\n",
        "NUM_EPISODES = 2000      # 최대 에피소드 수\n",
        "BATCH_SIZE = 32\n",
        "capacity = 10000        # Memory capacity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJO1aJ238-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 애니메이션을 만드는 함수\n",
        "# 참고 URL http://nbviewer.jupyter.org/github/patrickmineault\n",
        "# /xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
        "               dpi=72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
        "                                   interval=50)\n",
        "\n",
        "    # anim.save('movie_cartpole_DQN.mp4')  # 애니메이션을 저장하는 부분\n",
        "    anim.save('cartpole_DQN.gif', writer='ImageMagick', fps=60)\n",
        "    display(display_animation(anim, default_mode='loop'))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOb7UbfJ38-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    ''' Memory for random selection of trials '''\n",
        "    \n",
        "    def __init__(self, capacity: int) -> None:\n",
        "        self.capacity = capacity    # Memory capacity\n",
        "        self.memory = []            # Transition memory\n",
        "        self.index = 0              # indicate saving location\n",
        "        \n",
        "    def push(self, state: torch.FloatTensor, action: torch.LongTensor,\n",
        "             state_next: torch.FloatTensor, reward: torch.FloatTensor) -> None:\n",
        "        '''Transition~(s,a,s_n,r) 메모리 저장'''\n",
        "        if len(self.memory) < self.capacity:                                        # case memory not full\n",
        "            self.memory.append(None)                                                # increase size of list to avoid index error\n",
        "        \n",
        "        self.memory[self.index] = Transition(state, action, state_next, reward)     # add namedtuple Transition\n",
        "        self.index = (self.index + 1) % self.capacity                               # increase index (keep update over episode)\n",
        "        \n",
        "    def sample(self, batch_size: int) -> list:\n",
        "        '''replay 메모리에서 batch size 만큼 Transition 랜덤 뽑기'''\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        '''return saved Transitions'''\n",
        "        return len(self.memory)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self,n_in: int, n_mid: int, n_out: int):  \n",
        "        super(Net, self).__init__()  \n",
        "        self.fc1 = nn.Linear(n_in, n_mid)  \n",
        "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
        "        self.fc3 = nn.Linear(n.mid, n.out)  \n",
        "\n",
        "    def forward(self, x):  \n",
        "        x1 = F.relu(self.fc1(x))  \n",
        "        x2 = F.relu(self.fc2(x1))\n",
        "        out = self.fc3(x2)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPZaCESn38-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainNet:\n",
        "    ''' Where weights are optimized'''\n",
        "    def __init__(self,num_states: int, num_actions: int) -> None:\n",
        "        '''Initialize network models'''\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        \n",
        "        self.mem = ReplayMemory(capacity)                           # Initialize ReplayMem\n",
        "\n",
        "        n_in, n_mid, n_out = num_states, 32, num_actions\n",
        "        self.policy_net = Net(n_in, n_mid, n_out)                   # create policy(main) net\n",
        "        self.target_net = Net(n_in, n_mid, n_out)                   # create target net\n",
        "        print(self.model)                                           # print out the model\n",
        "        \n",
        "        # Selection of gradient descent model.(i.e. SGD, RMSprop, Adagrad,...)\n",
        "        # change weight coeffs of network with gradient descent of params\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.0001)\n",
        "\n",
        "    def replay(self) -> None:\n",
        "        '''REPLAY OF MEMORY & TRAIN OF NETWORK'''\n",
        "    # -----------------------------------------\n",
        "    # 1. 저장된 transition 수 확인\n",
        "    # -----------------------------------------\n",
        "        # 1.1 저장된 transition의 수가 미니배치 크기보다 작으면 아무 것도 하지 않음\n",
        "        if len(self.mem) < BATCH_SIZE:\n",
        "            return None\n",
        "        # 2. 미니배치 생성\n",
        "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_state = self.make_minibatch()\n",
        "\n",
        "        # 3. expected_Q 계산\n",
        "        self.expected_Q = self.get_expected_Q()\n",
        "\n",
        "        # 4. 결합 가중치 수정 (정책망 업데이트)\n",
        "        self.update_policynet()\n",
        "    \n",
        "\n",
        "    def make_minibatch(self) -> list\n",
        "    # -----------------------------------------\n",
        "    #  미니배치 생성\n",
        "    # -----------------------------------------\n",
        "        # 1 메모리 객체에서 미니배치를 추출\n",
        "        trans_sample = self.mem.sample(BATCH_SIZE)\n",
        "\n",
        "        # 2 각 변수를 미니배치에 맞는 형태로 변형\n",
        "        # trans_sample 은 각 단계 별로 (state, action, state_next, reward) 형태로 BATCH_SIZE 갯수만큼 저장됨\n",
        "        # 다시 말해, (state, action, state_next, reward) * BATCH_SIZE 형태가 된다\n",
        "        # 이것을 미니배치로 만들기 위해\n",
        "        # (state*BATCH_SIZE, action*BATCH_SIZE, state_next*BATCH_SIZE, reward*BATCH_SIZE) 형태로 변환한다\n",
        "        batch = Transition(*zip(*trans_sample))\n",
        "    \n",
        "        # 3 각 변수의 요소를 미니배치에 맞게 변형하고, 신경망으로 다룰 수 있도록 Variable로 만든다\n",
        "        # state를 예로 들면, [torch.FloatTensor of size 1*4] 형태의 요소가 BATCH_SIZE 갯수만큼 있는 형태이다\n",
        "        # 이를 torch.FloatTensor of size BATCH_SIZE*4 형태로 변형한다\n",
        "        # 상태, 행동, 보상, non_final 상태로 된 미니배치를 나타내는 Variable을 생성\n",
        "        # cat은 Concatenates을 의미한다\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_state = torch.cat([s for s in batch.next_state if s is not None])\n",
        "        return batch, state_batch, action_batch, reward_batch, non_final_next_state\n",
        "        \n",
        "    \n",
        "    def get_expected_Q(self):\n",
        "    # -----------------------------------------BBBBBBBBBBB\n",
        "    # 정답신호로 사용할 Q(s_t, a_t)를 계산\n",
        "    # -----------------------------------------\n",
        "        # 1 신경망을 추론 모드로 전환\n",
        "        self.policy_net.eval()\n",
        "        self.target_net.eval()\n",
        "\n",
        "        # 2 신경망으로 Q(s_t, a_t)를 계산\n",
        "        # self.model(state_batch)은 왼쪽, 오른쪽에 대한 Q값을 출력하며\n",
        "        # [torch.FloatTensor of size BATCH_SIZEx2] 형태이다\n",
        "        # 여기서부터는 실행한 행동 a_t에 대한 Q값을 계산하므로 action_batch에서 취한 행동 a_t가 \n",
        "        # 왼쪽이냐 오른쪽이냐에 대한 인덱스를 구하고, 이에 대한 Q값을 gather 메서드로 모아온다\n",
        "        self.Q = self.policy_net(self.state_batch).gather(1, self.action_batch)\n",
        "\n",
        "        # 3 max{Q(s_t+1, a)}값을 계산한다 이때 다음 상태가 존재하는지에 주의해야 한다\n",
        "        # cartpole이 done 상태가 아니고, next_state가 존재하는지 확인하는 인덱스 마스크를 만듬\n",
        "        # lamba & map -> https://offbyone.tistory.com/73\n",
        "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
        "\n",
        "        # 먼저 전체를 0으로 초기화\n",
        "        next_Q = torch.zeros(BATCH_SIZE)\n",
        "        action_max = torch.zeros(BATCH_SIZE).type(torch.LongTensor)    # 다음 상태에서 Q값이 최대가 되는 행동\n",
        "\n",
        "        # 다음 상태에서 Q값이 최대가 되는 행동 action_max을 Main Q-Network로 계산\n",
        "        # 마지막에 붙은 [1]로 행동에 해당하는 인덱스를 구함\n",
        "        action_max[non_final_mask] = self.policy_net(self.non_final_next_state).max(1)[1].detach()\n",
        "\n",
        "        # 다음 상태가 있는 것만을 걸러내고, size 32를 32*1로 변환\n",
        "        action_max_non_final_next_state = action_max[non_final_mask].view(-1, 1)\n",
        "        \n",
        "        # 다음 상태가 있는 인덱스에 대해 행동 action_max의 Q값을 target Q-Network로 계산\n",
        "        # policy net 에서 나타난 nextQmax 에 해당하는 action을 target net 으로 갖고와 따로 계산함.\n",
        "        # detach() 메서드로 값을 꺼내옴\n",
        "        # squeeze() 메서드로 size[minibatch*1]을 [minibatch]로 변환\n",
        "        next_Q[non_final_mask] = self.target_net(self.non_final_next_state).gather(1, action_max_non_final_next_state).detach().squeeze()\n",
        "\n",
        "        # 3.4 정답신호로 사용할 Q(s_t, a_t)값을 Q러닝 식으로 계산한다\n",
        "        expected_Q = self.eward_batch + GAMMA * next_Q\n",
        "\n",
        "        return expected_Q\n",
        "\n",
        "    def update_policynet(self):\n",
        "    # -----------------------------------------\n",
        "    #  결합 가중치 수정\n",
        "    # -----------------------------------------\n",
        "        # 1 신경망을 학습 모드로 전환\n",
        "        self.policy_net.train()\n",
        "\n",
        "        # 2 손실함수를 계산 (smooth_l1_loss는 Huber 함수)\n",
        "        # expected_Q는 size가 [minibatch]이므로 unsqueeze하여 [minibatch*1]로 만든다\n",
        "        loss = F.smooth_l1_loss(self.Q, self.expected_Q.unsqueeze(1))\n",
        "\n",
        "        # 3 결합 가중치를 수정한다\n",
        "        self.optimizer.zero_grad()      # Initialize gradient(경사)\n",
        "        loss.backward()                 # Calculate backward(역전파 계산)\n",
        "        self.optimizer.step()           # perform single optimize step (가중치 수정)\n",
        "    \n",
        "    def update_targetnet(self):\n",
        "    # -----------------------------------------\n",
        "    #  Target net 을 Policy net 과 동기화(주기적)\n",
        "    # -----------------------------------------\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())   # load_state_dict -> copies all state params & buffers from attribute\n",
        "\n",
        "\n",
        "    def decide_action(self, state: torch.FloatTensor, episode: int) -> torch.LongTensor:\n",
        "        '''Decision of action with \\epsilon greedy'''\n",
        "        epsilon = 0.5 * (1 / (episode + 1))                         # epsilon decay\n",
        "        if epsilon <= np.random.uniform(0,1):                           # weighted action\n",
        "            self.policy_net.eval()                                           # change network mode to inference\n",
        "            with torch.no_grad():\n",
        "                action = self.policy_net(state).max(1)[1].view(1,1)\n",
        "                # 신경망 출력의 최댓값에 대한 인덱스(action) = max(1)[1]\n",
        "                # .view(1,1)은 [torch.LongTensor of size 1] 을 size 1*1로 변환하는 역할을 한다\n",
        "\n",
        "        else:                                                           # random action\n",
        "            action = torch.LongTensor(\n",
        "                [[random.randrange(self.num_actions)]])                 # return 0 or 1 (LongTensor of size 1*1)\n",
        "\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "XgfbPLOY38-r",
        "colab_type": "code",
        "colab": {},
        "outputId": "9f716b12-36ff-47e7-eeef-48de83bd5488"
      },
      "source": [
        "class Environment:\n",
        "    ''' Initialize and run the environment '''\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        self.env = gym.make(ENV)                                # env set\n",
        "        num_states = self.env.observation_space.shape[0]        # Get State shape (4)\n",
        "        num_actions = self.env.action_space.n                   # Get action numbers (2)\n",
        "        self.train = TrainNet(num_states,num_actions)          # Initialize Network Class\n",
        "\n",
        "        \n",
        "    def run(self) -> None:\n",
        "        '''Run and update iteration'''\n",
        "        episode_10_list = np.zeros(10)                          # Save steps succeeded for last 10 episodes\n",
        "        complete_episodes = 0                                   # Episodes number that reached goal\n",
        "        is_episode_final = False                                # did it succeeded for 10 episodes? (terminate)\n",
        "        frames = []                                             # frame for animation\n",
        "\n",
        "        for episode in range(NUM_EPISODES):                         # Episode iteration (single train per episode)\n",
        "            observation = self.env.reset()                          # reset env (=initialize)\n",
        "            state = observation                                     # state <- initialized env\n",
        "            state = torch.from_numpy(state).type(\n",
        "                torch.FloatTensor)                                  # convert np.array -> FloatTensor\n",
        "            state = torch.unsqueeze(state, 0)                       # size 4 -> size 1*4 convert\n",
        "\n",
        "            for step in range(MAX_STEPS):                           # Iterate through max action(or state) per episode\n",
        "                \n",
        "                if is_episode_final is True:                              # If the goal reached\n",
        "                    frames.append(self.env.render(\n",
        "                        mode = 'rgb_array'))                              # Save result of final episode with frame\n",
        "                \n",
        "                action = self.network.decide_action(state, episode) # determine action through eps-greedy\n",
        "\n",
        "                new_observation, _, done, _ = self.env.step(\n",
        "                    action.item())                                  # get new state from decided action (reward & info -> blank)\n",
        "\n",
        "                if done:                                                # if pole lean down || iteration > max_episode_steps \n",
        "                    new_state = None                                    # No new state\n",
        "                    episode_10_list = np.hstack(\n",
        "                        (episode_10_list[1:], step + 1))                # save succeeded steps by every 'episode'\n",
        "                    \n",
        "                    if step < (MAX_STEPS - 5):                                # if pole lean down( < max_episode_steps)\n",
        "                        reward = torch.FloatTensor([-1.0])                  # reward -> -1\n",
        "                        complete_episodes = 0                               # reset complete_episode\n",
        "                    else:                                                   # if pole does not lean down\n",
        "                        reward = torch.FloatTensor([1.0])                   # reward -> 1\n",
        "                        complete_episodes += 1                              # update succeeded number\n",
        "                else:                                                   # not done(still in interation)\n",
        "                    reward = torch.FloatTensor([0.0])                   # reward = 0\n",
        "                    new_state = new_observation                         # update new state\n",
        "                    new_state = torch.from_numpy(new_state).type(           # change type to FloatTensor\n",
        "                        torch.FloatTensor)\n",
        "                    new_state = torch.unsqueeze(new_state, 0)           # size 4 -> size 1*4 convert\n",
        "\n",
        "                self.network.mem.push(\n",
        "                    state, action, new_state, reward)               # store iteration information(Transition) into memory\n",
        "                self.network.replay()                               # update Q with Experience Replay\n",
        "                state = new_state                                   # update state\n",
        "\n",
        "                if done:\n",
        "                    print(f'{episode} Episode: Finished after {step + 1} steps：최근 10 에피소드의 평균 단계 수 = {episode_10_list.mean()}')\n",
        "                \n",
        "                    if (episode % 2 == 0):                                  # Every 2 episode, after done of step, update the target net to policy net\n",
        "                        self.train.update_targetnet()\n",
        "                    break\n",
        "\n",
        "            if is_episode_final is True:\n",
        "                display_frames_as_gif(frames)                   # save result as gif\n",
        "                break\n",
        "\n",
        "            if complete_episodes >= 10:                         # (iteration > max_episode_steps) && (pole does not lean down) for 10 episodes\n",
        "                print('10 에피소드 연속 성공')\n",
        "                is_episode_final = True                        # save result and break\n",
        "                \n",
        "        if is_episode_final is False:                       #print if failed\n",
        "            print('학습 실패')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n",
            "0 Episode: Finished after 12 steps：최근 10 에피소드의 평균 단계 수 = 1.2\n",
            "1 Episode: Finished after 12 steps：최근 10 에피소드의 평균 단계 수 = 2.4\n",
            "2 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 3.4\n",
            "3 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 4.4\n",
            "4 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 5.4\n",
            "5 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 6.3\n",
            "6 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 7.2\n",
            "7 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 8.3\n",
            "8 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n",
            "9 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 10.2\n",
            "10 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 10.0\n",
            "11 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n",
            "12 Episode: Finished after 12 steps：최근 10 에피소드의 평균 단계 수 = 10.0\n",
            "13 Episode: Finished after 15 steps：최근 10 에피소드의 평균 단계 수 = 10.5\n",
            "14 Episode: Finished after 16 steps：최근 10 에피소드의 평균 단계 수 = 11.1\n",
            "15 Episode: Finished after 13 steps：최근 10 에피소드의 평균 단계 수 = 11.5\n",
            "16 Episode: Finished after 19 steps：최근 10 에피소드의 평균 단계 수 = 12.5\n",
            "17 Episode: Finished after 17 steps：최근 10 에피소드의 평균 단계 수 = 13.1\n",
            "18 Episode: Finished after 17 steps：최근 10 에피소드의 평균 단계 수 = 13.8\n",
            "19 Episode: Finished after 16 steps：최근 10 에피소드의 평균 단계 수 = 14.5\n",
            "20 Episode: Finished after 16 steps：최근 10 에피소드의 평균 단계 수 = 15.1\n",
            "21 Episode: Finished after 13 steps：최근 10 에피소드의 평균 단계 수 = 15.4\n",
            "22 Episode: Finished after 15 steps：최근 10 에피소드의 평균 단계 수 = 15.7\n",
            "23 Episode: Finished after 15 steps：최근 10 에피소드의 평균 단계 수 = 15.7\n",
            "24 Episode: Finished after 15 steps：최근 10 에피소드의 평균 단계 수 = 15.6\n",
            "25 Episode: Finished after 14 steps：최근 10 에피소드의 평균 단계 수 = 15.7\n",
            "26 Episode: Finished after 16 steps：최근 10 에피소드의 평균 단계 수 = 15.4\n",
            "27 Episode: Finished after 17 steps：최근 10 에피소드의 평균 단계 수 = 15.4\n",
            "28 Episode: Finished after 15 steps：최근 10 에피소드의 평균 단계 수 = 15.2\n",
            "29 Episode: Finished after 21 steps：최근 10 에피소드의 평균 단계 수 = 15.7\n",
            "30 Episode: Finished after 19 steps：최근 10 에피소드의 평균 단계 수 = 16.0\n",
            "31 Episode: Finished after 18 steps：최근 10 에피소드의 평균 단계 수 = 16.5\n",
            "32 Episode: Finished after 17 steps：최근 10 에피소드의 평균 단계 수 = 16.7\n",
            "33 Episode: Finished after 15 steps：최근 10 에피소드의 평균 단계 수 = 16.7\n",
            "34 Episode: Finished after 26 steps：최근 10 에피소드의 평균 단계 수 = 17.8\n",
            "35 Episode: Finished after 18 steps：최근 10 에피소드의 평균 단계 수 = 18.2\n",
            "36 Episode: Finished after 29 steps：최근 10 에피소드의 평균 단계 수 = 19.5\n",
            "37 Episode: Finished after 27 steps：최근 10 에피소드의 평균 단계 수 = 20.5\n",
            "38 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 20.1\n",
            "39 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 18.9\n",
            "40 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 17.8\n",
            "41 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 17.0\n",
            "42 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 16.2\n",
            "43 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 15.7\n",
            "44 Episode: Finished after 90 steps：최근 10 에피소드의 평균 단계 수 = 22.1\n",
            "45 Episode: Finished after 116 steps：최근 10 에피소드의 평균 단계 수 = 31.9\n",
            "46 Episode: Finished after 65 steps：최근 10 에피소드의 평균 단계 수 = 35.5\n",
            "47 Episode: Finished after 53 steps：최근 10 에피소드의 평균 단계 수 = 38.1\n",
            "48 Episode: Finished after 40 steps：최근 10 에피소드의 평균 단계 수 = 41.0\n",
            "49 Episode: Finished after 109 steps：최근 10 에피소드의 평균 단계 수 = 51.0\n",
            "50 Episode: Finished after 46 steps：최근 10 에피소드의 평균 단계 수 = 54.8\n",
            "51 Episode: Finished after 42 steps：최근 10 에피소드의 평균 단계 수 = 58.0\n",
            "52 Episode: Finished after 37 steps：최근 10 에피소드의 평균 단계 수 = 60.8\n",
            "53 Episode: Finished after 27 steps：최근 10 에피소드의 평균 단계 수 = 62.5\n",
            "54 Episode: Finished after 46 steps：최근 10 에피소드의 평균 단계 수 = 58.1\n",
            "55 Episode: Finished after 93 steps：최근 10 에피소드의 평균 단계 수 = 55.8\n",
            "56 Episode: Finished after 44 steps：최근 10 에피소드의 평균 단계 수 = 53.7\n",
            "57 Episode: Finished after 36 steps：최근 10 에피소드의 평균 단계 수 = 52.0\n",
            "58 Episode: Finished after 35 steps：최근 10 에피소드의 평균 단계 수 = 51.5\n",
            "59 Episode: Finished after 56 steps：최근 10 에피소드의 평균 단계 수 = 46.2\n",
            "60 Episode: Finished after 84 steps：최근 10 에피소드의 평균 단계 수 = 50.0\n",
            "61 Episode: Finished after 71 steps：최근 10 에피소드의 평균 단계 수 = 52.9\n",
            "62 Episode: Finished after 73 steps：최근 10 에피소드의 평균 단계 수 = 56.5\n",
            "63 Episode: Finished after 199 steps：최근 10 에피소드의 평균 단계 수 = 73.7\n",
            "64 Episode: Finished after 77 steps：최근 10 에피소드의 평균 단계 수 = 76.8\n",
            "65 Episode: Finished after 40 steps：최근 10 에피소드의 평균 단계 수 = 71.5\n",
            "66 Episode: Finished after 72 steps：최근 10 에피소드의 평균 단계 수 = 74.3\n",
            "67 Episode: Finished after 59 steps：최근 10 에피소드의 평균 단계 수 = 76.6\n",
            "68 Episode: Finished after 95 steps：최근 10 에피소드의 평균 단계 수 = 82.6\n",
            "69 Episode: Finished after 81 steps：최근 10 에피소드의 평균 단계 수 = 85.1\n",
            "70 Episode: Finished after 96 steps：최근 10 에피소드의 평균 단계 수 = 86.3\n",
            "71 Episode: Finished after 68 steps：최근 10 에피소드의 평균 단계 수 = 86.0\n",
            "72 Episode: Finished after 94 steps：최근 10 에피소드의 평균 단계 수 = 88.1\n",
            "73 Episode: Finished after 74 steps：최근 10 에피소드의 평균 단계 수 = 75.6\n",
            "74 Episode: Finished after 143 steps：최근 10 에피소드의 평균 단계 수 = 82.2\n",
            "75 Episode: Finished after 127 steps：최근 10 에피소드의 평균 단계 수 = 90.9\n",
            "76 Episode: Finished after 317 steps：최근 10 에피소드의 평균 단계 수 = 115.4\n",
            "77 Episode: Finished after 57 steps：최근 10 에피소드의 평균 단계 수 = 115.2\n",
            "78 Episode: Finished after 65 steps：최근 10 에피소드의 평균 단계 수 = 112.2\n",
            "79 Episode: Finished after 146 steps：최근 10 에피소드의 평균 단계 수 = 118.7\n",
            "80 Episode: Finished after 68 steps：최근 10 에피소드의 평균 단계 수 = 115.9\n",
            "81 Episode: Finished after 659 steps：최근 10 에피소드의 평균 단계 수 = 175.0\n",
            "82 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 235.6\n",
            "83 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 298.2\n",
            "84 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 353.9\n",
            "85 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 411.2\n",
            "86 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 449.5\n",
            "87 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 513.8\n",
            "88 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 577.3\n",
            "89 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 632.7\n",
            "90 Episode: Finished after 663 steps：최근 10 에피소드의 평균 단계 수 = 692.2\n",
            "91 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 696.3\n",
            "92 Episode: Finished after 463 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "93 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "94 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "95 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "96 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "97 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "98 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "99 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 672.6\n",
            "100 Episode: Finished after 441 steps：최근 10 에피소드의 평균 단계 수 = 650.4\n",
            "101 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 650.4\n",
            "102 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "103 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "104 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "105 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "106 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "107 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "108 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "109 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 674.1\n",
            "110 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 700.0\n",
            "10 에피소드 연속 성공\n",
            "111 Episode: Finished after 700 steps：최근 10 에피소드의 평균 단계 수 = 700.0\n",
            "MovieWriter stderr:\n",
            "convert-im6.q16: no images defined `cartpole_DQN.gif' @ error/convert.c/ConvertImageCommand/3258.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command '['convert', '-size', '600x400', '-depth', '8', '-delay', '1.6666666666666667', '-loop', '0', 'rgba:-', 'cartpole_DQN.gif']' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1144\u001b[0m                         \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mgrab_frame\u001b[0;34m(self, **savefig_kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         self.fig.savefig(self._frame_sink(), format=self.frame_format,\n\u001b[0;32m--> 364\u001b[0;31m                          dpi=self.dpi, **savefig_kwargs)\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2216\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2218\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_raw\u001b[0;34m(self, filename_or_obj, *args)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4b95eca6bfea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mcartpole_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mcartpole_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-4b95eca6bfea>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_episode_final\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mdisplay_frames_as_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# save result as gif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b62382d716e2>\u001b[0m in \u001b[0;36mdisplay_frames_as_gif\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# anim.save('movie_cartpole_DQN.mp4')  # 애니메이션을 저장하는 부분\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cartpole_DQN.gif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagemagick'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1143\u001b[0m                         \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                         \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;34m\"\"\"Finish any processing for writing the movie.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/pyt/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             raise subprocess.CalledProcessError(\n\u001b[0;32m--> 391\u001b[0;31m                 self._proc.returncode, self._proc.args, out, err)\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['convert', '-size', '600x400', '-depth', '8', '-delay', '1.6666666666666667', '-loop', '0', 'rgba:-', 'cartpole_DQN.gif']' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KLKjehKJm5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cartpole_env = Environment()\n",
        "cartpole_env.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}