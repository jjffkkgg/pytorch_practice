{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To develope from tutorial A2C\n",
    "- Enable GPU calc\n",
    "- Another model net (ex) CNN\n",
    "- print & save result\n",
    "- export net and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1130,
     "status": "error",
     "timestamp": 1599567495461,
     "user": {
      "displayName": "Taehwan Seo",
      "photoUrl": "",
      "userId": "06395709366113538068"
     },
     "user_tz": -540
    },
    "id": "ReU7Cds5c_NS",
    "outputId": "e28e84e1-0315-4938-c9ca-3ab754e0bcda"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR1Z3B23eJZR"
   },
   "outputs": [],
   "source": [
    "# set env with setting (especially reward and max episode)\n",
    "gym.envs.register(\n",
    "    id='CartPole_prefer-v0',\n",
    "    entry_point='gym.envs.classic_control:CartPoleEnv',\n",
    "    max_episode_steps=700,      # CartPole-v0 uses 200\n",
    "    reward_threshold=-110.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjKI9y2ed3TF"
   },
   "outputs": [],
   "source": [
    "'''Global Variables'''\n",
    "ENV = 'CartPole_prefer-v0'  # 태스크 이름\n",
    "GAMMA = 0.99                # 시간할인율\n",
    "MAX_STEPS = 700             # 1에피소드 당 최대 단계 수\n",
    "NUM_EPISODES = 2000         # 최대 에피소드 수\n",
    "\n",
    "NUM_PROCESSES = 32          # 동시 실행 환경 수\n",
    "NUM_ADVANCED_STEP = 5       # 총 보상을 계산할 때 Advantage 학습(action actor)을 할 단계 수\n",
    "\n",
    "VALUE_LOSS_COEFF = 0.5\n",
    "ENTROPY_COEFF = 0.01        # Local min 에서 벗어나기 위한 엔트로피 상수\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# GPU를 사용할 경우\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gx3uSUlDemU3"
   },
   "outputs": [],
   "source": [
    "class RolloutStorage(object):\n",
    "    '''Advantage 학습에 사용할 메모리 클래스'''\n",
    "    def __init__(self, num_steps: int, num_processes: int, obs_shape: int) -> None:\n",
    "        '''initialize tensors for work'''\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, obs_shape)        # obs tensor init\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)                        # mask -> is end of episode ? 0 | 1\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
    "\n",
    "        # 할인 총 보상(J(theta,s_t))저장하는 메모리 (Actor)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.index = 0              # insert 하는 index          \n",
    "\n",
    "    def insert(self, current_obs: tensor, action: tensor, reward: tensor, mask: FloatTensor) -> None:\n",
    "        '''현재 인덱스 위치에 transition을 저장'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 값 업데이트\n",
    "\n",
    "    def after_update(self):\n",
    "        '''Advantage학습 단계만큼 단계가 진행되면 가장 새로운 transition을 index0에 저장'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value):\n",
    "        '''Advantage학습 범위 안의 각 단계에 대해 할인 총보상(J)을 계산'''\n",
    "\n",
    "        # 주의 : 5번째 단계부터 거슬러 올라오며 계산\n",
    "        # 주의 : 5번째 단계가 Advantage1, 4번째 단계는 Advantage2가 됨\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            # advantage 고려 Q함수의 수정\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1928,
     "status": "error",
     "timestamp": 1599567608082,
     "user": {
      "displayName": "Taehwan Seo",
      "photoUrl": "",
      "userId": "06395709366113538068"
     },
     "user_tz": -540
    },
    "id": "J_dMPRXZem3w",
    "outputId": "866bb726-43ca-4217-b65d-5c121a52aca5"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in: int, n_mid: int, n_out: int) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in,n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid,n_mid)\n",
    "        self.actor = nn.Linear(n_mid, n_out)        # Actor net(two action output)\n",
    "        self.critic = nn.Linear(n_mid, 1)           # critic net (state value -> 1)\n",
    "    \n",
    "    def forward(self, x) -> list:\n",
    "        '''Forward wave 계산을 정의'''\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        critic_output = self.critic(h2)\n",
    "        actor_output = self.actor(h2)\n",
    "\n",
    "        return critic_output, actor_output\n",
    "    \n",
    "    def act(self, x) -> Tensor:\n",
    "        '''상태 x로부터 행동을 확률적으로 결정'''\n",
    "        value, actor_output = self(x)\n",
    "        action_probs = F.softmax(actor_output, dim=1)       # softmax 를 통한 행동의 확률 계산\n",
    "        action = action_probs.multinomial(num_samples=1)\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        '''상태 x로부터 상태가치를 계산'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        return value\n",
    "    \n",
    "    def evaluate_actions(self, x: tensor, actions) -> list:\n",
    "        '''상태 x로부터 상태가치, 실제 행동 actions의 로그 확률, 엔트로피를 계산'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(actor_output, dim=1)      # dim=1이므로 행동의 종류에 대해 확률을 계산\n",
    "        action_log_probs = log_probs.gather(1, actions)    # 실제 행동의 로그 확률(log_probs)을 구함\n",
    "\n",
    "        probs = F.softmax(actor_output, dim=1)\n",
    "        entropy = -(log_probs * probs).sum(-1).mean()\n",
    "\n",
    "        return value, action_log_probs, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zx1k8jyneoyx"
   },
   "outputs": [],
   "source": [
    "class Brain(object):\n",
    "    def __init__(self, actor_critic: Net) -> None:\n",
    "        self.actor_critic = actor_critic\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)    # learning rate = 0.01 (faster)\n",
    "        \n",
    "    def update(self, rollouts: RolloutStorage) -> None:\n",
    "        ''''Advantage학습의 대상이 되는 5단계 모두를 사용하여 수정'''\n",
    "        obs_shape = rollouts.observations.size()[2:]    # torch.Size([4, 84, 84])\n",
    "        num_steps = NUM_ADVANCED_STEP\n",
    "        num_processes = NUM_PROCESSES\n",
    "\n",
    "        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n",
    "            rollouts.observations[:-1].view(-1,4),\n",
    "            rollouts.actions.view(-1,1)\n",
    "        )\n",
    "\n",
    "        # 주의 : 각 변수의 크기\n",
    "        # rollouts.observations[:-1].view(-1, 4) torch.Size([80, 4])\n",
    "        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n",
    "        # values torch.Size([80, 1])\n",
    "        # action_log_probs torch.Size([80, 1])\n",
    "        # entropy torch.Size([])\n",
    "\n",
    "        values = values.view(num_steps, num_processes, 1)   # torch.Size([5, 16, 1])\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        # advantage(행동가치-상태가치) 계산\n",
    "        advantages = rollouts.returns[:-1] - values          # torch.Size([5, 16, 1])\n",
    "\n",
    "        # Critic loss 계산\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # Actor의 gain 계산, 나중에 -1을 곱하면 loss가 된다\n",
    "        action_gain = (action_log_probs * advantages.detach()).mean()\n",
    "        # detach 메서드를 호출하여 advantages를 상수로 취급\n",
    "\n",
    "        # 오차함수의 총합\n",
    "        total_loss = (value_loss * VALUE_LOSS_COEFF - action_gain - entropy * ENTROPY_COEFF)\n",
    "        \n",
    "        # 가중치 수정\n",
    "        self.actor_critic.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        # 결합 가중치가 한번에 너무 크게 변화하지 않도록, 경사를 0.5 이하로 제한함(클리핑)\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pl6ubIxseq-Y"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def run(self) -> None:\n",
    "        '''running entry point'''\n",
    "        # 동시 실행할 환경 수 만큼 env를 생성\n",
    "        envs = [gym.make(ENV) for i in range(NUM_PROCESSES)]\n",
    "\n",
    "        # 모든 에이전트가 공유하는 Brain 객체를 생성\n",
    "        n_in = envs[0].observation_space.shape[0]           # state inputs\n",
    "        n_out = envs[0].action_space.n                      # action outpus\n",
    "        n_mid = 32                                          # 32 mid junction\n",
    "        actor_critic = Net(n_in, n_mid, n_out).to(DEVICE)   # Net init\n",
    "        glob_brain = Brain(actor_critic)                    # Brain init\n",
    "\n",
    "        # 각종 정보를 저장하는 변수\n",
    "        obs_shape = n_in\n",
    "        current_obs = torch.zeros(NUM_PROCESSES, obs_shape)                         # (16,4) 의 tensor\n",
    "        rollouts = RolloutStorage(NUM_ADVANCED_STEP, NUM_PROCESSES,obs_shape)       # RolloutStorage init\n",
    "        episode_rewards = torch.zeros(NUM_PROCESSES, 1)                             # 현재 episode 의 reward\n",
    "        final_rewards = torch.zeros(NUM_PROCESSES, 1)                               # 마지막 episode 의 reward\n",
    "        obs_np = np.zeros([NUM_PROCESSES, obs_shape])                               # state 배열\n",
    "        reward_np = np.zeros([NUM_PROCESSES, 1])                                    # 보상의 배열\n",
    "        done_np = np.zeros([NUM_PROCESSES, 1])                                      # Done 여부의 배열\n",
    "        each_step = np.zeros(NUM_PROCESSES)                                         # 각 env 의 step record\n",
    "        episode = 0\n",
    "        # 초기 state...\n",
    "        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n",
    "        obs = np.array(obs)\n",
    "        obs = torch.from_numpy(obs).float()                     # (16,4) 의 tensor\n",
    "        current_obs = obs                                       # current obs 의 업데이트\n",
    "\n",
    "        # advanced 학습(action actor)에 사용되는 객체 rollouts 첫번째 상태에 현재 상태를 저장\n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "        # 에피소드 반복문\n",
    "        for episode in range(NUM_PROCESSES * NUM_EPISODES):\n",
    "            # advanced 학습(action actor) 대상이 되는 각 단계에 대해 계산 (step 반복)\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "                # action 을 fetch\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "                # (16,1)→(16,) -> tensor를 NumPy변수로\n",
    "                actions = action.squeeze(1).numpy()\n",
    "\n",
    "                # process 반복\n",
    "                for i in range(NUM_PROCESSES):\n",
    "                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(actions[i])\n",
    "\n",
    "                    # episode의 종료가치, state_next를 설정\n",
    "                    if done_np[i]:          # 지정된 step 달성 혹은 무너짐\n",
    "                        if i == 0:          # 0번째의 환경 결과만 출력\n",
    "                            print(f'{episode} Episode: Finished after {each_step[i] + 1} steps')\n",
    "                        episode += 1\n",
    "                        # 보상 부여\n",
    "                        if each_step[i] < (MAX_STEPS - 5):\n",
    "                            reward_np[i] = -1.0     # 무너졌을때\n",
    "                        else:\n",
    "                            reward_np[i] = 1.0      # 성공했을때\n",
    "                        \n",
    "                        each_step[i] = 0            # step 초기화\n",
    "                        obs_np[i] = envs[i].reset() # 환경 초기화\n",
    "                    else:                           # 무너지거나 성공도 아님\n",
    "                        reward_np[i] = 0.0\n",
    "                        each_step[i] += 1           # 그대로 진행\n",
    "\n",
    "                # 보상을 tensor로 변환하고, 에피소드의 총보상에 더해줌\n",
    "                reward = torch.from_numpy(reward_np).float()\n",
    "                episode_rewards += reward\n",
    "\n",
    "                # 각 실행 환경을 확인하여 done이 true이면 mask를 0으로, false이면 mask를 1로\n",
    "                masks = torch.FloatTensor([[0.0] if done_i else [1.0] for done_i in done_np])\n",
    "\n",
    "                # 마지막 에피소드의 총 보상을 업데이트\n",
    "                final_rewards *= masks      # done이 false이면 1을 곱하고, true이면 0을 곱해 초기화\n",
    "                # done이 false이면 0을 더하고, true이면 episode_rewards를 더해줌\n",
    "                final_rewards += (1 - masks)*episode_rewards\n",
    "\n",
    "                # 에피소드의 총보상을 업데이트\n",
    "                episode_rewards *= masks\n",
    "\n",
    "                # 현재 done이 true이면 모두 0으로 \n",
    "                current_obs *= masks\n",
    "\n",
    "                # current_obs를 업데이트\n",
    "                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\n",
    "                current_obs = obs  # 최신 상태의 obs를 저장\n",
    "\n",
    "                # 메모리 객체에 현 단계의 transition을 저장\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "\n",
    "            # advanced 학습 for문 끝\n",
    "\n",
    "            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(rollouts.observations[-1]).detach()\n",
    "                # next_value는 다음 state의 상태가치\n",
    "                # rollouts.observations의 크기는 torch.Size([6, 16, 4])\n",
    "            \n",
    "            # 모든 단계의 할인총보상을 계산하고, rollouts의 변수 returns를 업데이트\n",
    "            rollouts.compute_returns(next_value)\n",
    "\n",
    "            # 신경망 및 rollout 업데이트\n",
    "            glob_brain.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "\n",
    "            # 모든 환경이 성공\n",
    "            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n",
    "                print('모든 환경 성공')\n",
    "                torch.save(actor_critic.state_dict(), \"./\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0nn2cr_exWL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Episode: Finished after 43.0 steps\n",
      "16 Episode: Finished after 24.0 steps\n",
      "23 Episode: Finished after 38.0 steps\n",
      "34 Episode: Finished after 58.0 steps\n",
      "38 Episode: Finished after 24.0 steps\n",
      "42 Episode: Finished after 20.0 steps\n",
      "93 Episode: Finished after 262.0 steps\n",
      "107 Episode: Finished after 59.0 steps\n",
      "130 Episode: Finished after 122.0 steps\n",
      "160 Episode: Finished after 148.0 steps\n",
      "280 Episode: Finished after 606.0 steps\n",
      "342 Episode: Finished after 309.0 steps\n",
      "381 Episode: Finished after 197.0 steps\n",
      "413 Episode: Finished after 156.0 steps\n",
      "459 Episode: Finished after 230.0 steps\n",
      "534 Episode: Finished after 364.0 steps\n",
      "536 Episode: Finished after 21.0 steps\n",
      "591 Episode: Finished after 271.0 steps\n",
      "632 Episode: Finished after 210.0 steps\n",
      "635 Episode: Finished after 14.0 steps\n",
      "703 Episode: Finished after 339.0 steps\n",
      "720 Episode: Finished after 87.0 steps\n",
      "723 Episode: Finished after 16.0 steps\n",
      "751 Episode: Finished after 129.0 steps\n",
      "754 Episode: Finished after 11.0 steps\n",
      "754 Episode: Finished after 14.0 steps\n",
      "761 Episode: Finished after 17.0 steps\n",
      "761 Episode: Finished after 17.0 steps\n",
      "778 Episode: Finished after 85.0 steps\n",
      "798 Episode: Finished after 92.0 steps\n",
      "820 Episode: Finished after 115.0 steps\n",
      "856 Episode: Finished after 184.0 steps\n",
      "892 Episode: Finished after 181.0 steps\n",
      "934 Episode: Finished after 208.0 steps\n",
      "977 Episode: Finished after 211.0 steps\n",
      "998 Episode: Finished after 105.0 steps\n",
      "1002 Episode: Finished after 24.0 steps\n",
      "1046 Episode: Finished after 211.0 steps\n",
      "1063 Episode: Finished after 71.0 steps\n",
      "1071 Episode: Finished after 54.0 steps\n",
      "1099 Episode: Finished after 147.0 steps\n",
      "1127 Episode: Finished after 139.0 steps\n",
      "1139 Episode: Finished after 65.0 steps\n",
      "1159 Episode: Finished after 97.0 steps\n",
      "1197 Episode: Finished after 192.0 steps\n",
      "1236 Episode: Finished after 176.0 steps\n",
      "1296 Episode: Finished after 314.0 steps\n",
      "1301 Episode: Finished after 11.0 steps\n",
      "1354 Episode: Finished after 279.0 steps\n",
      "1422 Episode: Finished after 341.0 steps\n",
      "1476 Episode: Finished after 274.0 steps\n",
      "1522 Episode: Finished after 229.0 steps\n",
      "1551 Episode: Finished after 138.0 steps\n",
      "1586 Episode: Finished after 186.0 steps\n",
      "1627 Episode: Finished after 197.0 steps\n",
      "1649 Episode: Finished after 116.0 steps\n",
      "1671 Episode: Finished after 108.0 steps\n",
      "1809 Episode: Finished after 692.0 steps\n",
      "1827 Episode: Finished after 92.0 steps\n",
      "1847 Episode: Finished after 100.0 steps\n",
      "1871 Episode: Finished after 116.0 steps\n",
      "1904 Episode: Finished after 164.0 steps\n",
      "1925 Episode: Finished after 109.0 steps\n",
      "1947 Episode: Finished after 107.0 steps\n",
      "2026 Episode: Finished after 399.0 steps\n",
      "2064 Episode: Finished after 186.0 steps\n",
      "2085 Episode: Finished after 103.0 steps\n",
      "2102 Episode: Finished after 87.0 steps\n",
      "2116 Episode: Finished after 51.0 steps\n",
      "2120 Episode: Finished after 36.0 steps\n",
      "2126 Episode: Finished after 32.0 steps\n",
      "2133 Episode: Finished after 33.0 steps\n",
      "2145 Episode: Finished after 47.0 steps\n",
      "2150 Episode: Finished after 34.0 steps\n",
      "2162 Episode: Finished after 67.0 steps\n",
      "2183 Episode: Finished after 105.0 steps\n",
      "2197 Episode: Finished after 62.0 steps\n",
      "2208 Episode: Finished after 65.0 steps\n",
      "2265 Episode: Finished after 287.0 steps\n",
      "2294 Episode: Finished after 143.0 steps\n",
      "2326 Episode: Finished after 158.0 steps\n",
      "2349 Episode: Finished after 115.0 steps\n",
      "2376 Episode: Finished after 133.0 steps\n",
      "2394 Episode: Finished after 92.0 steps\n",
      "2406 Episode: Finished after 60.0 steps\n",
      "2426 Episode: Finished after 89.0 steps\n",
      "2437 Episode: Finished after 67.0 steps\n",
      "2462 Episode: Finished after 87.0 steps\n",
      "2465 Episode: Finished after 9.0 steps\n",
      "2461 Episode: Finished after 9.0 steps\n",
      "2465 Episode: Finished after 10.0 steps\n",
      "2462 Episode: Finished after 9.0 steps\n",
      "2464 Episode: Finished after 10.0 steps\n",
      "2466 Episode: Finished after 10.0 steps\n",
      "2482 Episode: Finished after 8.0 steps\n",
      "2482 Episode: Finished after 11.0 steps\n",
      "2484 Episode: Finished after 9.0 steps\n",
      "2485 Episode: Finished after 10.0 steps\n",
      "2487 Episode: Finished after 11.0 steps\n",
      "2478 Episode: Finished after 12.0 steps\n",
      "2485 Episode: Finished after 16.0 steps\n",
      "2490 Episode: Finished after 12.0 steps\n",
      "2526 Episode: Finished after 211.0 steps\n",
      "2529 Episode: Finished after 16.0 steps\n",
      "2534 Episode: Finished after 10.0 steps\n",
      "2536 Episode: Finished after 15.0 steps\n",
      "2545 Episode: Finished after 17.0 steps\n",
      "2544 Episode: Finished after 18.0 steps\n",
      "2548 Episode: Finished after 31.0 steps\n",
      "2556 Episode: Finished after 32.0 steps\n",
      "2562 Episode: Finished after 40.0 steps\n",
      "2576 Episode: Finished after 55.0 steps\n",
      "2587 Episode: Finished after 71.0 steps\n",
      "2602 Episode: Finished after 39.0 steps\n",
      "2607 Episode: Finished after 50.0 steps\n",
      "2610 Episode: Finished after 27.0 steps\n",
      "2622 Episode: Finished after 41.0 steps\n",
      "2632 Episode: Finished after 65.0 steps\n",
      "2655 Episode: Finished after 119.0 steps\n",
      "2700 Episode: Finished after 225.0 steps\n",
      "2715 Episode: Finished after 72.0 steps\n",
      "2720 Episode: Finished after 24.0 steps\n",
      "2744 Episode: Finished after 120.0 steps\n",
      "2794 Episode: Finished after 253.0 steps\n",
      "2805 Episode: Finished after 14.0 steps\n",
      "2799 Episode: Finished after 11.0 steps\n",
      "2802 Episode: Finished after 15.0 steps\n",
      "2809 Episode: Finished after 11.0 steps\n",
      "2817 Episode: Finished after 12.0 steps\n",
      "2809 Episode: Finished after 12.0 steps\n",
      "2818 Episode: Finished after 8.0 steps\n",
      "2813 Episode: Finished after 12.0 steps\n",
      "2820 Episode: Finished after 11.0 steps\n",
      "2817 Episode: Finished after 9.0 steps\n",
      "2830 Episode: Finished after 17.0 steps\n",
      "2830 Episode: Finished after 10.0 steps\n",
      "2825 Episode: Finished after 13.0 steps\n",
      "2834 Episode: Finished after 8.0 steps\n",
      "2836 Episode: Finished after 15.0 steps\n",
      "2838 Episode: Finished after 10.0 steps\n",
      "2836 Episode: Finished after 9.0 steps\n",
      "2843 Episode: Finished after 12.0 steps\n",
      "2845 Episode: Finished after 9.0 steps\n",
      "2842 Episode: Finished after 13.0 steps\n",
      "2845 Episode: Finished after 11.0 steps\n",
      "2845 Episode: Finished after 13.0 steps\n",
      "2849 Episode: Finished after 11.0 steps\n",
      "2852 Episode: Finished after 10.0 steps\n",
      "2855 Episode: Finished after 10.0 steps\n",
      "2861 Episode: Finished after 17.0 steps\n",
      "2861 Episode: Finished after 13.0 steps\n",
      "2860 Episode: Finished after 14.0 steps\n",
      "2872 Episode: Finished after 17.0 steps\n",
      "2875 Episode: Finished after 12.0 steps\n",
      "2877 Episode: Finished after 19.0 steps\n",
      "2876 Episode: Finished after 11.0 steps\n",
      "2876 Episode: Finished after 12.0 steps\n",
      "2880 Episode: Finished after 15.0 steps\n",
      "2884 Episode: Finished after 13.0 steps\n",
      "2887 Episode: Finished after 22.0 steps\n",
      "2892 Episode: Finished after 17.0 steps\n",
      "2890 Episode: Finished after 12.0 steps\n",
      "2895 Episode: Finished after 19.0 steps\n",
      "2900 Episode: Finished after 17.0 steps\n",
      "2900 Episode: Finished after 14.0 steps\n",
      "2908 Episode: Finished after 24.0 steps\n",
      "2911 Episode: Finished after 29.0 steps\n",
      "2917 Episode: Finished after 25.0 steps\n",
      "2919 Episode: Finished after 20.0 steps\n",
      "2938 Episode: Finished after 93.0 steps\n",
      "2960 Episode: Finished after 110.0 steps\n",
      "2965 Episode: Finished after 15.0 steps\n",
      "2966 Episode: Finished after 17.0 steps\n",
      "2970 Episode: Finished after 14.0 steps\n",
      "2974 Episode: Finished after 23.0 steps\n",
      "2989 Episode: Finished after 68.0 steps\n",
      "3018 Episode: Finished after 143.0 steps\n",
      "3071 Episode: Finished after 232.0 steps\n",
      "3072 Episode: Finished after 10.0 steps\n",
      "3069 Episode: Finished after 10.0 steps\n",
      "3074 Episode: Finished after 11.0 steps\n",
      "3075 Episode: Finished after 19.0 steps\n",
      "3081 Episode: Finished after 41.0 steps\n",
      "3111 Episode: Finished after 154.0 steps\n",
      "3124 Episode: Finished after 62.0 steps\n",
      "3137 Episode: Finished after 66.0 steps\n",
      "3150 Episode: Finished after 66.0 steps\n",
      "3165 Episode: Finished after 71.0 steps\n",
      "3188 Episode: Finished after 121.0 steps\n",
      "3217 Episode: Finished after 141.0 steps\n",
      "3237 Episode: Finished after 101.0 steps\n",
      "3259 Episode: Finished after 108.0 steps\n",
      "3284 Episode: Finished after 122.0 steps\n",
      "3343 Episode: Finished after 283.0 steps\n",
      "3393 Episode: Finished after 259.0 steps\n",
      "3426 Episode: Finished after 173.0 steps\n",
      "3466 Episode: Finished after 199.0 steps\n",
      "3512 Episode: Finished after 230.0 steps\n",
      "3555 Episode: Finished after 218.0 steps\n",
      "3658 Episode: Finished after 513.0 steps\n",
      "3743 Episode: Finished after 424.0 steps\n",
      "3813 Episode: Finished after 348.0 steps\n",
      "3876 Episode: Finished after 296.0 steps\n",
      "3899 Episode: Finished after 137.0 steps\n",
      "3923 Episode: Finished after 115.0 steps\n",
      "3926 Episode: Finished after 12.0 steps\n",
      "3930 Episode: Finished after 19.0 steps\n",
      "3946 Episode: Finished after 85.0 steps\n",
      "3963 Episode: Finished after 75.0 steps\n",
      "3964 Episode: Finished after 19.0 steps\n",
      "3981 Episode: Finished after 86.0 steps\n",
      "4013 Episode: Finished after 159.0 steps\n",
      "4041 Episode: Finished after 139.0 steps\n",
      "4121 Episode: Finished after 401.0 steps\n",
      "4215 Episode: Finished after 468.0 steps\n",
      "4259 Episode: Finished after 224.0 steps\n",
      "4293 Episode: Finished after 166.0 steps\n",
      "4343 Episode: Finished after 253.0 steps\n",
      "4414 Episode: Finished after 351.0 steps\n",
      "4514 Episode: Finished after 503.0 steps\n",
      "4568 Episode: Finished after 272.0 steps\n",
      "4610 Episode: Finished after 208.0 steps\n",
      "4652 Episode: Finished after 210.0 steps\n",
      "4701 Episode: Finished after 245.0 steps\n",
      "4736 Episode: Finished after 168.0 steps\n",
      "4785 Episode: Finished after 252.0 steps\n",
      "4878 Episode: Finished after 462.0 steps\n",
      "4913 Episode: Finished after 177.0 steps\n",
      "4980 Episode: Finished after 328.0 steps\n",
      "5037 Episode: Finished after 290.0 steps\n",
      "5077 Episode: Finished after 202.0 steps\n",
      "5139 Episode: Finished after 314.0 steps\n",
      "5183 Episode: Finished after 218.0 steps\n",
      "5250 Episode: Finished after 322.0 steps\n",
      "5330 Episode: Finished after 399.0 steps\n",
      "5418 Episode: Finished after 452.0 steps\n",
      "5497 Episode: Finished after 398.0 steps\n",
      "5544 Episode: Finished after 235.0 steps\n",
      "5582 Episode: Finished after 187.0 steps\n",
      "5626 Episode: Finished after 220.0 steps\n",
      "5679 Episode: Finished after 265.0 steps\n",
      "5717 Episode: Finished after 183.0 steps\n",
      "5751 Episode: Finished after 165.0 steps\n",
      "5757 Episode: Finished after 20.0 steps\n",
      "5756 Episode: Finished after 17.0 steps\n",
      "5787 Episode: Finished after 157.0 steps\n",
      "5830 Episode: Finished after 212.0 steps\n",
      "5885 Episode: Finished after 257.0 steps\n",
      "5888 Episode: Finished after 28.0 steps\n",
      "5971 Episode: Finished after 421.0 steps\n",
      "6017 Episode: Finished after 222.0 steps\n",
      "6052 Episode: Finished after 185.0 steps\n",
      "6140 Episode: Finished after 437.0 steps\n",
      "6172 Episode: Finished after 150.0 steps\n",
      "6175 Episode: Finished after 13.0 steps\n",
      "6191 Episode: Finished after 81.0 steps\n",
      "6206 Episode: Finished after 91.0 steps\n",
      "6277 Episode: Finished after 353.0 steps\n",
      "6285 Episode: Finished after 39.0 steps\n",
      "6425 Episode: Finished after 700.0 steps\n",
      "6431 Episode: Finished after 31.0 steps\n",
      "6531 Episode: Finished after 494.0 steps\n",
      "6539 Episode: Finished after 47.0 steps\n",
      "6679 Episode: Finished after 700.0 steps\n",
      "6781 Episode: Finished after 511.0 steps\n",
      "6832 Episode: Finished after 253.0 steps\n",
      "6864 Episode: Finished after 159.0 steps\n",
      "7004 Episode: Finished after 700.0 steps\n",
      "7081 Episode: Finished after 381.0 steps\n",
      "7126 Episode: Finished after 224.0 steps\n",
      "7160 Episode: Finished after 168.0 steps\n",
      "7203 Episode: Finished after 223.0 steps\n",
      "7251 Episode: Finished after 239.0 steps\n",
      "7255 Episode: Finished after 20.0 steps\n",
      "7395 Episode: Finished after 700.0 steps\n",
      "7535 Episode: Finished after 700.0 steps\n",
      "7560 Episode: Finished after 125.0 steps\n",
      "7702 Episode: Finished after 700.0 steps\n",
      "7841 Episode: Finished after 700.0 steps\n",
      "7903 Episode: Finished after 315.0 steps\n",
      "8043 Episode: Finished after 700.0 steps\n",
      "8078 Episode: Finished after 177.0 steps\n",
      "8127 Episode: Finished after 246.0 steps\n",
      "8204 Episode: Finished after 374.0 steps\n",
      "8205 Episode: Finished after 12.0 steps\n",
      "8213 Episode: Finished after 12.0 steps\n",
      "8210 Episode: Finished after 13.0 steps\n",
      "8218 Episode: Finished after 16.0 steps\n",
      "8216 Episode: Finished after 14.0 steps\n",
      "8218 Episode: Finished after 10.0 steps\n",
      "8349 Episode: Finished after 653.0 steps\n",
      "8449 Episode: Finished after 501.0 steps\n",
      "8514 Episode: Finished after 330.0 steps\n",
      "8585 Episode: Finished after 350.0 steps\n",
      "8652 Episode: Finished after 340.0 steps\n",
      "8792 Episode: Finished after 700.0 steps\n",
      "8930 Episode: Finished after 690.0 steps\n",
      "9005 Episode: Finished after 371.0 steps\n",
      "9123 Episode: Finished after 591.0 steps\n",
      "9244 Episode: Finished after 607.0 steps\n",
      "9384 Episode: Finished after 700.0 steps\n",
      "9437 Episode: Finished after 253.0 steps\n",
      "9576 Episode: Finished after 700.0 steps\n",
      "9585 Episode: Finished after 41.0 steps\n",
      "9723 Episode: Finished after 698.0 steps\n",
      "9769 Episode: Finished after 233.0 steps\n",
      "9909 Episode: Finished after 700.0 steps\n",
      "9964 Episode: Finished after 270.0 steps\n",
      "10104 Episode: Finished after 700.0 steps\n",
      "10244 Episode: Finished after 700.0 steps\n",
      "10384 Episode: Finished after 700.0 steps\n",
      "10432 Episode: Finished after 246.0 steps\n",
      "10449 Episode: Finished after 81.0 steps\n",
      "10584 Episode: Finished after 677.0 steps\n",
      "10679 Episode: Finished after 473.0 steps\n",
      "10730 Episode: Finished after 254.0 steps\n",
      "10871 Episode: Finished after 700.0 steps\n",
      "10924 Episode: Finished after 272.0 steps\n",
      "10963 Episode: Finished after 197.0 steps\n",
      "11015 Episode: Finished after 244.0 steps\n",
      "11063 Episode: Finished after 241.0 steps\n",
      "11065 Episode: Finished after 22.0 steps\n",
      "11089 Episode: Finished after 123.0 steps\n",
      "11115 Episode: Finished after 131.0 steps\n",
      "11143 Episode: Finished after 136.0 steps\n",
      "11168 Episode: Finished after 125.0 steps\n",
      "11191 Episode: Finished after 111.0 steps\n",
      "11212 Episode: Finished after 107.0 steps\n",
      "11234 Episode: Finished after 112.0 steps\n",
      "11262 Episode: Finished after 141.0 steps\n",
      "11287 Episode: Finished after 118.0 steps\n",
      "11308 Episode: Finished after 114.0 steps\n",
      "11331 Episode: Finished after 114.0 steps\n",
      "11359 Episode: Finished after 134.0 steps\n",
      "11384 Episode: Finished after 128.0 steps\n",
      "11411 Episode: Finished after 136.0 steps\n",
      "11441 Episode: Finished after 154.0 steps\n",
      "11470 Episode: Finished after 142.0 steps\n",
      "11511 Episode: Finished after 207.0 steps\n",
      "11650 Episode: Finished after 693.0 steps\n",
      "11766 Episode: Finished after 581.0 steps\n",
      "11856 Episode: Finished after 452.0 steps\n",
      "11996 Episode: Finished after 700.0 steps\n",
      "12009 Episode: Finished after 57.0 steps\n",
      "12014 Episode: Finished after 32.0 steps\n",
      "12096 Episode: Finished after 407.0 steps\n",
      "12236 Episode: Finished after 700.0 steps\n",
      "12376 Episode: Finished after 700.0 steps\n",
      "12498 Episode: Finished after 610.0 steps\n",
      "12535 Episode: Finished after 185.0 steps\n",
      "12625 Episode: Finished after 451.0 steps\n",
      "12640 Episode: Finished after 71.0 steps\n",
      "12780 Episode: Finished after 700.0 steps\n",
      "모든 환경 성공\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cartpole_env = Environment()\n",
    "    cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "inv_pend_A2C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('pyt': conda)",
   "language": "python",
   "name": "python37864bitpytcondafd1b43a097a64d34bd7b82617eda6abc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}