{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"quadrotor_A2C.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOnTkC9KUNs2OPLSyNvKS1h"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oaxq54kCV8Eq","colab_type":"code","colab":{}},"source":["# from __future__ import annotations\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import torch\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import copy\n","from quadrotor import QuadRotorEnv \n","\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cge6863IWbtn","colab_type":"code","colab":{}},"source":["'''Global Variables'''\n","GAMMA = 0.99                # 시간할인율\n","MAX_STEPS = 2000             # 1에피소드 당 최대 단계 수 (0.01 second per step)\n","NUM_EPISODES = 10000         # 최대 에피소드 수\n","\n","NUM_PROCESSES = 32          # 동시 실행 환경 수\n","NUM_ADVANCED_STEP = 5       # 총 보상을 계산할 때 Advantage 학습(action actor)을 할 단계 수\n","\n","VALUE_LOSS_COEFF = 0.5\n","ENTROPY_COEFF = 0.01        # Local min 에서 벗어나기 위한 엔트로피 상수\n","MAX_GRAD_NORM = 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3t0PM06xWY0H","colab_type":"code","colab":{}},"source":["class RolloutStorage(object):\n","    '''Advantage 학습에 사용할 메모리 클래스'''\n","    def __init__(self, num_steps: int, num_processes: int, obs_shape: int) -> None:\n","        '''initialize tensors for work'''\n","        self.observations = torch.zeros(num_steps + 1, num_processes, obs_shape).to(device)        # obs tensor init\n","        self.masks = torch.ones(num_steps + 1, num_processes, 1).to(device)                        # mask -> is end of episode ? 0 | 1\n","        self.rewards = torch.zeros(num_steps, num_processes, 1).to(device)  \n","        self.actions = torch.zeros(num_steps, num_processes, 1).long().to(device)\n","\n","        # 할인 총 보상(J(theta,s_t))저장하는 메모리 (Actor)\n","        self.returns = torch.zeros(num_steps + 1, num_processes, 1).to(device)\n","        self.index = 0              # insert 하는 index          \n","\n","    def insert(self, current_obs: tensor, action: tensor, reward: tensor, mask: FloatTensor) -> None:\n","        '''현재 인덱스 위치에 transition을 저장'''\n","        self.observations[self.index + 1].copy_(current_obs)        # torch.size([6, 32, 12])\n","        self.masks[self.index + 1].copy_(mask)\n","        self.rewards[self.index].copy_(reward)\n","        self.actions[self.index].copy_(action)\n","\n","        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 값 업데이트\n","\n","    def after_update(self):\n","        '''Advantage학습 단계만큼 단계가 진행되면 가장 새로운 transition을 index0에 저장'''\n","        self.observations[0].copy_(self.observations[-1])\n","        self.masks[0].copy_(self.masks[-1])\n","\n","    def compute_returns(self, next_value):\n","        '''Advantage학습 범위 안의 각 단계에 대해 할인 총보상(J)을 계산'''\n","\n","        # 주의 : 5번째 단계부터 거슬러 올라오며 계산\n","        # 주의 : 5번째 단계가 Advantage1, 4번째 단계는 Advantage2가 됨\n","        self.returns[-1] = next_value\n","        for ad_step in reversed(range(self.rewards.size(0))):\n","            # advantage 고려 Q함수의 수정\n","            self.returns[ad_step] = self.returns[ad_step + 1] * GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJjDEg1eWWB2","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self, n_in: int, n_mid: int, n_out: int) -> None:\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(n_in,n_mid)\n","        self.fc2 = nn.Linear(n_mid,n_mid)\n","        self.actor = nn.Linear(n_mid, n_out)        # Actor net(8 action output)\n","        self.critic = nn.Linear(n_mid, 1)           # critic net (state value -> 1)\n","    \n","    def forward(self, x) -> list:\n","        '''Forward wave 계산을 정의'''\n","        h1 = F.relu(self.fc1(x))\n","        h2 = F.relu(self.fc2(h1))\n","        critic_output = self.critic(h2)\n","        actor_output = self.actor(h2)\n","\n","        return critic_output, actor_output\n","    \n","    def act(self, x) -> Tensor:\n","        '''상태 x로부터 행동을 확률적으로 결정'''\n","        value, actor_output = self(x)\n","        action_probs = F.softmax(actor_output, dim=1)       # softmax 를 통한 행동의 확률 계산\n","        action = action_probs.multinomial(num_samples=1)\n","        return action\n","\n","    def get_value(self, x):\n","        '''상태 x로부터 상태가치를 계산'''\n","        value, actor_output = self(x)\n","\n","        return value\n","    \n","    def evaluate_actions(self, x: tensor, actions) -> list:\n","        '''상태 x로부터 상태가치, 실제 행동 actions의 로그 확률, 엔트로피를 계산'''\n","        value, actor_output = self(x)\n","\n","        log_probs = F.log_softmax(actor_output, dim=1)      # dim=1이므로 행동의 종류에 대해 확률을 계산\n","        action_log_probs = log_probs.gather(1, actions)    # 실제 행동의 로그 확률(log_probs)을 구함\n","\n","        probs = F.softmax(actor_output, dim=1)\n","        entropy = -(log_probs * probs).sum(-1).mean()\n","\n","        return value, action_log_probs, entropy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oe9-uSwsWTB_","colab_type":"code","colab":{}},"source":["class Brain(object):\n","    def __init__(self, actor_critic: Net) -> None:\n","        self.actor_critic = actor_critic\n","        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)    # learning rate = 0.01 (faster)\n","        \n","    def update(self, rollouts: RolloutStorage) -> None:\n","        ''''Advantage학습의 대상이 되는 5단계 모두를 사용하여 수정'''\n","        obs_shape = rollouts.observations.size()[2:]    # torch.Size([12])\n","        num_steps = NUM_ADVANCED_STEP\n","        num_processes = NUM_PROCESSES\n","\n","        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n","            rollouts.observations[:-1].view(-1,12),\n","            rollouts.actions.view(-1,1)\n","        )\n","\n","        # 주의 : 각 변수의 크기\n","        # rollouts.observations[:-1].view(-1, 4) torch.Size([192, 12])\n","        # rollouts.actions.view(-1, 1) torch.Size([192, 8])\n","        # values torch.Size([80, 1])\n","        # action_log_probs torch.Size([80, 1])\n","        # entropy torch.Size([])\n","\n","        values = values.view(num_steps, num_processes, 1)   # torch.Size([5, 32, 1])\n","        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n","\n","        # advantage(행동가치-상태가치) 계산\n","        advantages = rollouts.returns[:-1] - values          # torch.Size([6, 32, 1])\n","\n","        # Critic loss 계산\n","        value_loss = advantages.pow(2).mean()\n","\n","        # Actor의 gain 계산, 나중에 -1을 곱하면 loss가 된다\n","        action_gain = (action_log_probs * advantages.detach()).mean()\n","        # detach 메서드를 호출하여 advantages를 상수로 취급\n","\n","        # 오차함수의 총합\n","        total_loss = (value_loss * VALUE_LOSS_COEFF - action_gain - entropy * ENTROPY_COEFF)\n","        \n","        # 가중치 수정\n","        self.actor_critic.train()\n","        self.optimizer.zero_grad()\n","        total_loss.backward()\n","        # 결합 가중치가 한번에 너무 크게 변화하지 않도록, 경사를 0.5 이하로 제한함(클리핑)\n","        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), MAX_GRAD_NORM)\n","\n","        self.optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlUZD52YWPMH","colab_type":"code","colab":{}},"source":["class Environment:\n","    def run(self) -> None:\n","        '''running entry point'''\n","        # 동시 실행할 환경 수 만큼 env를 생성\n","        envs = [QuadRotorEnv() for i in range(NUM_PROCESSES)]\n","\n","        # 모든 에이전트가 공유하는 Brain 객체를 생성\n","        n_in = envs[0].observation_space_size           # state inputs\n","        n_out = envs[0].action_space_size               # action outpus\n","        n_mid = 96                                      # 48 mid junction\n","        actor_critic = Net(n_in, n_mid, n_out).to(device)          # Net init\n","        glob_brain = Brain(actor_critic)                # Brain init\n","\n","        # 각종 정보를 저장하는 변수\n","        l_arm = 0.3                             # length or the rotor arm [m]\n","        m = 1.3                                 # mass of vehicle [kg]\n","        rho = 1.225                             # density of air [kg/m^3]\n","        r = 0.1                                 # radius of propeller\n","        V = 11.1                                # voltage of battery [V]\n","        kV = 1550                               # motor kV constant, [rpm/V]\n","        CT = 1.0e-2                             # Thrust coeff\n","        Cm = 1e-4                               # moment coeff\n","        g = 9.81                                # gravitational constant [m/s^2]\n","        Jx = 0.021                              # Moment of inertia Ixx [kg*m^2]\n","        Jy = 0.021                              # Moment of inertia Iyy [kg*m^2]\n","        Jz = 0.042                              # Moment of inertia Izz [kg*m^2]\n","        p = [m, l_arm, r, rho, V, kV, CT, Cm, g, Jx, Jy, Jz]\n","        obs_shape = n_in\n","        current_obs = torch.zeros(NUM_PROCESSES, obs_shape).to(device)                         # (16,4) 의 tensor\n","        rollouts = RolloutStorage(NUM_ADVANCED_STEP, NUM_PROCESSES,obs_shape)       # RolloutStorage init\n","        episode_rewards = torch.zeros(NUM_PROCESSES, 1)                             # 현재 episode 의 reward\n","        final_rewards = torch.zeros(NUM_PROCESSES, 1)                               # 마지막 episode 의 reward\n","        obs_np = np.zeros([NUM_PROCESSES, obs_shape])                               # state 배열\n","        reward_np = np.zeros([NUM_PROCESSES, 1])                                    # 보상의 배열\n","        done_np = np.zeros([NUM_PROCESSES, 1])                                      # Done 여부의 배열\n","        arrive_np = np.zeros([NUM_PROCESSES, 1])                                    # Arrive 여부의 배열\n","        arrive_time = []                                                            # Arrive time 의 저장 버퍼\n","        each_step = np.zeros(NUM_PROCESSES)                                         # 각 env 의 step record\n","        episode = 0\n","        # 초기 state...\n","        obs = [envs[i].reset(p) for i in range(NUM_PROCESSES)]\n","        obs = np.array(obs)\n","        obs = torch.from_numpy(obs).float()                     # (32,12) 의 tensor\n","        current_obs = obs                                       # current obs 의 업데이트\n","\n","        # advanced 학습(action actor)에 사용되는 객체 rollouts 첫번째 상태에 현재 상태를 저장\n","        rollouts.observations[0].copy_(current_obs)\n","\n","        # 에피소드 반복문\n","        for episode in range(NUM_PROCESSES * NUM_EPISODES):\n","            # advanced 학습(action actor) 대상이 되는 각 단계에 대해 계산 (step 반복)\n","            for step in range(NUM_ADVANCED_STEP):\n","                # action 을 fetch\n","                with torch.no_grad():\n","                    action = actor_critic.act(rollouts.observations[step])\n","                # (16,1)→(16,) -> tensor를 NumPy변수로\n","                actions = action.squeeze(1).numpy()\n","\n","                # process 반복\n","                for i in range(NUM_PROCESSES):\n","                    obs_np[i], reward_np[i], done_np[i], arrive_np[i] = envs[i].step(actions[i], each_step[i])\n","\n","                    # episode의 종료가치, state_next를 설정\n","                    if done_np[i]:          # success or fail\n","                        print(f'{episode}: {(each_step[i] + 1)/100} [s]')\n","                        episode += 1\n","                        if arrive_np[i]:    # done with arrival\n","                            print('arrived!')\n","                            if len(arrive_time) == 0:   # First arrival\n","                                reward_np[i] = 20.0\n","                                arrive_time.append(each_step[i] / 100)\n","                            else:\n","                                for j in arrive_time:\n","                                    if j > (each_step[i] / 100):    # better record of success\n","                                        arrive_time.append(each_step[i] / 100)\n","                                        reward_np[i] = 20.0\n","                                    else:                           # poor or same performace to last success\n","                                        reward_np[i] = 0.0\n","                        else:\n","                            reward_np[i] = -10.0\n","                        each_step[i] = 0.0            # step 초기화\n","                        obs_np[i] = envs[i].reset(p) # 환경 초기화\n","                    else:                           # 무너지거나 성공도 아님\n","                        reward_np[i] = 0.0\n","                        each_step[i] += 1           # 그대로 진행\n","\n","                # 보상을 tensor로 변환하고, 에피소드의 총보상에 더해줌\n","                reward = torch.from_numpy(reward_np).float()\n","                episode_rewards += reward\n","\n","                # 각 실행 환경을 확인하여 done이 true이면 mask를 0으로, false이면 mask를 1로\n","                masks = torch.FloatTensor([[0.0] if done_i else [1.0] for done_i in done_np])\n","\n","                # 마지막 에피소드의 총 보상을 업데이트\n","                final_rewards *= masks      # done이 false이면 1을 곱하고, true이면 0을 곱해 초기화\n","                # done이 false이면 0을 더하고, true이면 episode_rewards를 더해줌\n","                final_rewards += (1 - masks)*episode_rewards\n","\n","                # 에피소드의 총보상을 업데이트\n","                episode_rewards *= masks\n","\n","                # 현재 done이 true이면 모두 0으로 \n","                current_obs *= masks\n","\n","                # current_obs를 업데이트\n","                obs = torch.from_numpy(obs_np).float()  # torch.Size([32, 12])\n","                current_obs = obs  # 최신 상태의 obs를 저장\n","\n","                # 메모리 객체에 현 단계의 transition을 저장\n","                rollouts.insert(current_obs, action.data, reward, masks)\n","\n","            # advanced 학습 for문 끝\n","\n","            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n","\n","            with torch.no_grad():\n","                next_value = actor_critic.get_value(rollouts.observations[-1]).detach()\n","                # next_value는 다음 state의 상태가치\n","                # rollouts.observations의 크기는 torch.Size([6, 16, 4])\n","            \n","            # 모든 단계의 할인총보상을 계산하고, rollouts의 변수 returns를 업데이트\n","            rollouts.compute_returns(next_value)\n","\n","            # 신경망 및 rollout 업데이트\n","            glob_brain.update(rollouts)\n","            rollouts.after_update()\n","\n","            # 모든 환경이 성공\n","            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n","                print('모든 환경 성공')\n","                break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zj7nhaUaWJLs","colab_type":"code","colab":{}},"source":["if __name__ == '__main__':\n","    cartpole_env = Environment()\n","    cartpole_env.run()"],"execution_count":null,"outputs":[]}]}