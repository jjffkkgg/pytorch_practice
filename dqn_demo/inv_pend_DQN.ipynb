{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv = gym.make(\\'CartPole-v0\\')\\nenv.reset()\\nrandom_episodes = 0\\nreward_sum = 0\\nwhile random_episodes < 10:\\n    env.render()\\n    action = env.action_space.sample()\\n    observation, reward, done, _ = env.step(action)\\n    print(observation, reward, done)\\n    reward_sum += reward\\n    if done:\\n        random_episodes += 1\\n        print(f\"Reward for this episode was: {reward_sum}\")\\n        reward_sum = 0\\n        env.reset()\\nenv.close()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    print(observation, reward, done)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(f\"Reward for this episode was: {reward_sum}\")\n",
    "        reward_sum = 0\n",
    "        env.reset()\n",
    "env.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# set env with setting (especially reward and max episode)\n",
    "gym.envs.register(\n",
    "    id='CartPole_prefer-v0',\n",
    "    entry_point='gym.envs.classic_control:CartPole',\n",
    "    max_episode_steps=500,      # CartPole-v0 uses 200\n",
    "    reward_threshold=-110.0,\n",
    ")\n",
    "\n",
    "# 상수 정의\n",
    "ENV = 'CartPole_prefer-v0'     # 태스크 이름\n",
    "GAMMA = 0.99            # 시간할인율\n",
    "MAX_STEPS = 200         # 1에피소드 당 최대 단계 수\n",
    "NUM_EPISODES = 500      # 최대 에피소드 수\n",
    "batch_size = 32\n",
    "capacity = 10000        # Memory capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 애니메이션을 만드는 함수\n",
    "# 참고 URL http://nbviewer.jupyter.org/github/patrickmineault\n",
    "# /xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole_DQN.mp4')  # 애니메이션을 저장하는 부분\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    ''' Memory for random selection of trials '''\n",
    "    \n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity    # Memory capacity\n",
    "        self.memory = []            # Transition memory\n",
    "        self.index = 0              # indicate saving location\n",
    "        \n",
    "    def push(self, state: torch.FloatTensor, action: torch.LongTensor,\n",
    "             state_next: torch.FloatTensor, reward: torch.FloatTensor) -> None:\n",
    "        '''Transition~(s,a,s_n,r) 메모리 저장'''\n",
    "        if len(self.memory) < self.capacity:                                        # case memory not full\n",
    "            self.memory.append(None)                                                # increase size of list to avoid index error\n",
    "        \n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)     # add namedtuple Transition\n",
    "        self.index = (self.index + 1) % self.capacity                               # increase index (??? index init?)\n",
    "        \n",
    "    def sample(self, batch_size: int) -> list:\n",
    "        '''replay 메모리에서 batch size 만큼 Transition 랜덤 뽑기'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        '''return saved Transitions'''\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-2d147e8b142e>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2d147e8b142e>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    def decide_action(self, state: torch.FloatTensor, episode: int) -> torch.LongTensor:\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    ''' Where deep network formed and optimized'''\n",
    "    def __init__(self,num_states: int, num_actions: int) -> None:\n",
    "        '''Initialize network models'''\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.mem = ReplayMemory(capacity)                           # Initialize ReplayMem\n",
    "\n",
    "        self.model = nn.Sequential()                                # Sequential container of modules(networks) \n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))     # Fully-connected(FC) model with input of state output of 32\n",
    "        self.model.add_module('relu1', nn.ReLU())                   # Backward diff wave alg (RELU typically)\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))             # Second module, modules can also have CNN, RNN, etc..\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))    # Goes through network with output of action\n",
    "\n",
    "        print(self.model)                                           # print out the model\n",
    "        \n",
    "        # Selection of gradient descent model.(i.e. SGD, RMSprop, Adagrad,...)\n",
    "        # change weight coeffs of network with gradient descent of params\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.0001)\n",
    "        \n",
    "    def replay(self) -> None:\n",
    "        '''REPLAY OF MEMORY & TRAIN OF NETWORK'''\n",
    "\n",
    "        \n",
    "    def decide_action(self, state: torch.FloatTensor, episode: int) -> torch.LongTensor:\n",
    "        '''Decision of action with \\epsilon greedy'''\n",
    "        epsilon = 0.5 * (1 / (episode + 1))                         # epsilon decay\n",
    "        if epsilon <= np.random.uniform(0,1):                           # weighted action\n",
    "            self.model.eval()                                           # change network mode to inference\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1,1)\n",
    "                # 신경망 출력의 최댓값에 대한 인덱스(action) = max(1)[1]\n",
    "                # .view(1,1)은 [torch.LongTensor of size 1] 을 size 1*1로 변환하는 역할을 한다\n",
    "\n",
    "        else:                                                           # random action\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])                 # return 0 or 1 (LongTensor of size 1*1)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    ''' Initialize and run the environment '''\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.env = gym.make(ENV)                                # env set\n",
    "        num_states = self.env.observation_space.shape[0]        # Get State shape (4)\n",
    "        num_actions = self.env.action_space.n                   # Get action numbers (2)\n",
    "        self.network = Network(num_states,num_actions)          # Initialize Network Class\n",
    "\n",
    "        \n",
    "    def run(self) -> None:\n",
    "        '''Run and update iteration'''\n",
    "        episode_10_list = np.zeros(10)                          # Save steps succeeded for last 10 episodes\n",
    "        complete_episodes = 0                                   # Episodes number that reached goal\n",
    "        is_episode_final = False                                # did it succeeded for 10 episodes? (terminate)\n",
    "        frames = []                                             # frame for animation\n",
    "\n",
    "        for episode in range(NUM_EPISODES):                         # Episode iteration (single train per episode)\n",
    "            observation = self.env.reset()                          # reset env (=initialize)\n",
    "            state = observation                                     # state <- initialized env\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)                                  # convert np.array -> FloatTensor\n",
    "            state = torch.unsqueeze(state, 0)                       # size 4 -> size 1*4 convert\n",
    "\n",
    "            for step in range(MAX_STEPS):                           # Iterate through max action(or state) per episode\n",
    "                \n",
    "                if is_episode_final is True:                              # If the goal reached\n",
    "                    frames.append(self.env.render(\n",
    "                        mode = 'rgb_array'))                              # Save result of final episode with frame\n",
    "                \n",
    "                action = self.network.decide_action(state, episode) # determine action through eps-greedy\n",
    "\n",
    "                new_observation, _, done, _ = self.env.step(\n",
    "                    action.item())                                  # get new state from decided action (reward & info -> blank)\n",
    "\n",
    "                if done:                                                # if pole lean down|| iteration > max_episode_steps \n",
    "                    new_state = None                                    # No new state\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))                # save succeeded steps by every 'episode'\n",
    "                    \n",
    "                    if step < 495:                                          # if pole lean down( < max_episode_steps)\n",
    "                        reward = torch.FloatTensor([-1.0])                  # reward -> -1\n",
    "                        complete_episodes = 0                               # reset complete_episode\n",
    "                    else:                                                   # if pole does not lean down\n",
    "                        reward = torch.FloatTensor([1.0])                   # reward -> 1\n",
    "                        complete_episodes += 1                              # update succeeded number\n",
    "                else:                                                   # not done(still in interation)\n",
    "                    reward = torch.FloatTensor([0.0])                   # reward = 0\n",
    "                    new_state = new_observation                         # update new state\n",
    "                    new_state = torch.from_numpy(state).type(           # change type to FloatTensor\n",
    "                        torch.FloatTensor)\n",
    "                    new_state = torch.unsqueeze(new_state, 0)           # size 4 -> size 1*4 convert\n",
    "\n",
    "                self.network.mem.push(\n",
    "                    state, action, new_state, reward)               # store iteration information(Transition) into memory\n",
    "                self.network.replay()                               # update Q with Experience Replay\n",
    "                state = new_state                                   # update state\n",
    "\n",
    "                if done:\n",
    "                    print(f'{episode} Episode: Finished after {step + 1} steps：최근 10 에피소드의 평균 단계 수 = {episode_10_list.mean()}')\n",
    "                    break\n",
    "\n",
    "            if is_episode_final is True:\n",
    "                display_frames_as_gif(frames)                   # save result as gif\n",
    "                break\n",
    "\n",
    "            if complete_episodes >= 10:                         # (iteration > max_episode_steps) && (pole does not lean down) for 10 episodes\n",
    "                print('10 에피소드 연속 성공')\n",
    "                is_episodes_final = True                        # save result and break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mDocstring:\u001b[0m\nuniform(low=0.0, high=1.0, size=None)\n\nDraw samples from a uniform distribution.\n\nSamples are uniformly distributed over the half-open interval\n``[low, high)`` (includes low, but excludes high).  In other words,\nany value within the given interval is equally likely to be drawn\nby `uniform`.\n\n.. note::\n    New code should use the ``uniform`` method of a ``default_rng()``\n    instance instead; see `random-quick-start`.\n\nParameters\n----------\nlow : float or array_like of floats, optional\n    Lower boundary of the output interval.  All values generated will be\n    greater than or equal to low.  The default value is 0.\nhigh : float or array_like of floats\n    Upper boundary of the output interval.  All values generated will be\n    less than or equal to high.  The default value is 1.0.\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n    a single value is returned if ``low`` and ``high`` are both scalars.\n    Otherwise, ``np.broadcast(low, high).size`` samples are drawn.\n\nReturns\n-------\nout : ndarray or scalar\n    Drawn samples from the parameterized uniform distribution.\n\nSee Also\n--------\nrandint : Discrete uniform distribution, yielding integers.\nrandom_integers : Discrete uniform distribution over the closed\n                  interval ``[low, high]``.\nrandom_sample : Floats uniformly distributed over ``[0, 1)``.\nrandom : Alias for `random_sample`.\nrand : Convenience function that accepts dimensions as input, e.g.,\n       ``rand(2,2)`` would generate a 2-by-2 array of floats,\n       uniformly distributed over ``[0, 1)``.\nGenerator.uniform: which should be used for new code.\n\nNotes\n-----\nThe probability density function of the uniform distribution is\n\n.. math:: p(x) = \\frac{1}{b - a}\n\nanywhere within the interval ``[a, b)``, and zero elsewhere.\n\nWhen ``high`` == ``low``, values of ``low`` will be returned.\nIf ``high`` < ``low``, the results are officially undefined\nand may eventually raise an error, i.e. do not rely on this\nfunction to behave when passed arguments satisfying that\ninequality condition. The ``high`` limit may be included in the\nreturned array of floats due to floating-point rounding in the\nequation ``low + (high-low) * random_sample()``. For example:\n\n>>> x = np.float32(5*0.99999999)\n>>> x\n5.0\n\n\nExamples\n--------\nDraw samples from the distribution:\n\n>>> s = np.random.uniform(-1,0,1000)\n\nAll values are within the given interval:\n\n>>> np.all(s >= -1)\nTrue\n>>> np.all(s < 0)\nTrue\n\nDisplay the histogram of the samples, along with the\nprobability density function:\n\n>>> import matplotlib.pyplot as plt\n>>> count, bins, ignored = plt.hist(s, 15, density=True)\n>>> plt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\n>>> plt.show()\n\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
    }
   ],
   "source": [
    "np.random.uniform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('pyt': conda)",
   "language": "python",
   "name": "python37864bitpytcondafd1b43a097a64d34bd7b82617eda6abc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}