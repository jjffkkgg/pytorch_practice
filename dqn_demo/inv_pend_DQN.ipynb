{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nenv = gym.make(\\'CartPole-v0\\')\\nenv.reset()\\nrandom_episodes = 0\\nreward_sum = 0\\nwhile random_episodes < 10:\\n    env.render()\\n    action = env.action_space.sample()\\n    observation, reward, done, _ = env.step(action)\\n    print(observation, reward, done)\\n    reward_sum += reward\\n    if done:\\n        random_episodes += 1\\n        print(f\"Reward for this episode was: {reward_sum}\")\\n        reward_sum = 0\\n        env.reset()\\nenv.close()\\n'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    print(observation, reward, done)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        random_episodes += 1\n",
    "        print(f\"Reward for this episode was: {reward_sum}\")\n",
    "        reward_sum = 0\n",
    "        env.reset()\n",
    "env.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# set env with setting (especially reward and max episode)\n",
    "gym.envs.register(\n",
    "    id='CartPole_prefer-v0',\n",
    "    entry_point='gym.envs.classic_control:CartPoleEnv',\n",
    "    max_episode_steps=500,      # CartPole-v0 uses 200\n",
    "    reward_threshold=-110.0,\n",
    ")\n",
    "\n",
    "# 상수 정의\n",
    "ENV = 'CartPole_prefer-v0'     # 태스크 이름\n",
    "GAMMA = 0.99            # 시간할인율\n",
    "MAX_STEPS = 500         # 1에피소드 당 최대 단계 수\n",
    "NUM_EPISODES = 1000      # 최대 에피소드 수\n",
    "batch_size = 32\n",
    "capacity = 10000        # Memory capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 애니메이션을 만드는 함수\n",
    "# 참고 URL http://nbviewer.jupyter.org/github/patrickmineault\n",
    "# /xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole_DQN.mp4')  # 애니메이션을 저장하는 부분\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    ''' Memory for random selection of trials '''\n",
    "    \n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity    # Memory capacity\n",
    "        self.memory = []            # Transition memory\n",
    "        self.index = 0              # indicate saving location\n",
    "        \n",
    "    def push(self, state: torch.FloatTensor, action: torch.LongTensor,\n",
    "             state_next: torch.FloatTensor, reward: torch.FloatTensor) -> None:\n",
    "        '''Transition~(s,a,s_n,r) 메모리 저장'''\n",
    "        if len(self.memory) < self.capacity:                                        # case memory not full\n",
    "            self.memory.append(None)                                                # increase size of list to avoid index error\n",
    "        \n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)     # add namedtuple Transition\n",
    "        self.index = (self.index + 1) % self.capacity                               # increase index (keep update over episode)\n",
    "        \n",
    "    def sample(self, batch_size: int) -> list:\n",
    "        '''replay 메모리에서 batch size 만큼 Transition 랜덤 뽑기'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        '''return saved Transitions'''\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    ''' Where deep network formed and optimized'''\n",
    "    def __init__(self,num_states: int, num_actions: int) -> None:\n",
    "        '''Initialize network models'''\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.mem = ReplayMemory(capacity)                           # Initialize ReplayMem\n",
    "\n",
    "        self.model = nn.Sequential()                                # Sequential container of modules(networks) \n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))     # Fully-connected(FC) model with input of state output of 32\n",
    "        self.model.add_module('relu1', nn.ReLU())                   # Backward diff wave alg (RELU typically)\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))             # Second module, modules can also have CNN, RNN, etc..\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))    # Goes through network with output of action\n",
    "\n",
    "        print(self.model)                                           # print out the model\n",
    "        \n",
    "        # Selection of gradient descent model.(i.e. SGD, RMSprop, Adagrad,...)\n",
    "        # change weight coeffs of network with gradient descent of params\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.0001)\n",
    "        \n",
    "    def replay(self) -> None:\n",
    "        '''REPLAY OF MEMORY & TRAIN OF NETWORK'''\n",
    "    # -----------------------------------------\n",
    "    # 1. 저장된 transition 수 확인\n",
    "    # -----------------------------------------\n",
    "        # 1.1 저장된 transition의 수가 미니배치 크기보다 작으면 아무 것도 하지 않음\n",
    "        if len(self.mem) < batch_size:\n",
    "            return None\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 2. 미니배치 생성\n",
    "    # -----------------------------------------\n",
    "        # 2.1 메모리 객체에서 미니배치를 추출\n",
    "        trans_sample = self.mem.sample(batch_size)\n",
    "\n",
    "        # 2.2 각 변수를 미니배치에 맞는 형태로 변형\n",
    "        # trans_sample 은 각 단계 별로 (state, action, state_next, reward) 형태로 BATCH_SIZE 갯수만큼 저장됨\n",
    "        # 다시 말해, (state, action, state_next, reward) * BATCH_SIZE 형태가 된다\n",
    "        # 이것을 미니배치로 만들기 위해\n",
    "        # (state*BATCH_SIZE, action*BATCH_SIZE, state_next*BATCH_SIZE, reward*BATCH_SIZE) 형태로 변환한다\n",
    "        batch = Transition(*zip(*trans_sample))\n",
    "    \n",
    "        # 2.3 각 변수의 요소를 미니배치에 맞게 변형하고, 신경망으로 다룰 수 있도록 Variable로 만든다\n",
    "        # state를 예로 들면, [torch.FloatTensor of size 1*4] 형태의 요소가 BATCH_SIZE 갯수만큼 있는 형태이다\n",
    "        # 이를 torch.FloatTensor of size BATCH_SIZE*4 형태로 변형한다\n",
    "        # 상태, 행동, 보상, non_final 상태로 된 미니배치를 나타내는 Variable을 생성\n",
    "        # cat은 Concatenates을 의미한다\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_state = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 3. 정답신호로 사용할 Q(s_t, a_t)를 계산\n",
    "    # -----------------------------------------\n",
    "        # 3.1 신경망을 추론 모드로 전환\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 신경망으로 Q(s_t, a_t)를 계산\n",
    "        # self.model(state_batch)은 왼쪽, 오른쪽에 대한 Q값을 출력하며\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2] 형태이다\n",
    "        # 여기서부터는 실행한 행동 a_t에 대한 Q값을 계산하므로 action_batch에서 취한 행동 a_t가 \n",
    "        # 왼쪽이냐 오른쪽이냐에 대한 인덱스를 구하고, 이에 대한 Q값을 gather 메서드로 모아온다\n",
    "        Q = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}값을 계산한다 이때 다음 상태가 존재하는지에 주의해야 한다\n",
    "        # cartpole이 done 상태가 아니고, next_state가 존재하는지 확인하는 인덱스 마스크를 만듬\n",
    "        # lamba & map -> https://offbyone.tistory.com/73\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "\n",
    "        # 먼저 전체를 0으로 초기화\n",
    "        next_Q = torch.zeros(batch_size)\n",
    "\n",
    "        # 다음 상태가 있는 인덱스에 대한 최대 Q값을 구한다\n",
    "        # 출력에 접근하여 열방향 최대값(max(1))이 되는 [값, 인덱스]를 구한다\n",
    "        # 그리고 이 Q값(인덱스=0)을 출력한 다음\n",
    "        # detach 메서드로 이 값을 꺼내온다\n",
    "        next_Q[non_final_mask] = self.model(non_final_next_state).max(1)[0].detach()\n",
    "        # 3.4 정답신호로 사용할 Q(s_t, a_t)값을 Q러닝 식으로 계산한다\n",
    "        expected_Q = reward_batch + GAMMA * next_Q\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 4. 결합 가중치 수정\n",
    "    # -----------------------------------------\n",
    "        # 4.1 신경망을 학습 모드로 전환\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 손실함수를 계산 (smooth_l1_loss는 Huber 함수)\n",
    "        # expected_Q는 size가 [minibatch]이므로 unsqueeze하여 [minibatch*1]로 만든다\n",
    "        loss = F.smooth_l1_loss(Q, expected_Q.unsqueeze(1))\n",
    "\n",
    "        # 4.3 결합 가중치를 수정한다\n",
    "        self.optimizer.zero_grad()      # Initialize gradient(경사)\n",
    "        loss.backward()                 # Calculate backward(역전파 계산)\n",
    "        self.optimizer.step()           # perform single optimize step (가중치 수정)\n",
    "\n",
    "    def decide_action(self, state: torch.FloatTensor, episode: int) -> torch.LongTensor:\n",
    "        '''Decision of action with \\epsilon greedy'''\n",
    "        epsilon = 0.5 * (1 / (episode + 1))                         # epsilon decay\n",
    "        if epsilon <= np.random.uniform(0,1):                           # weighted action\n",
    "            self.model.eval()                                           # change network mode to inference\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1,1)\n",
    "                # 신경망 출력의 최댓값에 대한 인덱스(action) = max(1)[1]\n",
    "                # .view(1,1)은 [torch.LongTensor of size 1] 을 size 1*1로 변환하는 역할을 한다\n",
    "\n",
    "        else:                                                           # random action\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])                 # return 0 or 1 (LongTensor of size 1*1)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    ''' Initialize and run the environment '''\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.env = gym.make(ENV)                                # env set\n",
    "        num_states = self.env.observation_space.shape[0]        # Get State shape (4)\n",
    "        num_actions = self.env.action_space.n                   # Get action numbers (2)\n",
    "        self.network = Network(num_states,num_actions)          # Initialize Network Class\n",
    "\n",
    "        \n",
    "    def run(self) -> None:\n",
    "        '''Run and update iteration'''\n",
    "        episode_10_list = np.zeros(10)                          # Save steps succeeded for last 10 episodes\n",
    "        complete_episodes = 0                                   # Episodes number that reached goal\n",
    "        is_episode_final = False                                # did it succeeded for 10 episodes? (terminate)\n",
    "        frames = []                                             # frame for animation\n",
    "\n",
    "        for episode in range(NUM_EPISODES):                         # Episode iteration (single train per episode)\n",
    "            observation = self.env.reset()                          # reset env (=initialize)\n",
    "            state = observation                                     # state <- initialized env\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)                                  # convert np.array -> FloatTensor\n",
    "            state = torch.unsqueeze(state, 0)                       # size 4 -> size 1*4 convert\n",
    "\n",
    "            for step in range(MAX_STEPS):                           # Iterate through max action(or state) per episode\n",
    "                \n",
    "                if is_episode_final is True:                              # If the goal reached\n",
    "                    frames.append(self.env.render(\n",
    "                        mode = 'rgb_array'))                              # Save result of final episode with frame\n",
    "                \n",
    "                action = self.network.decide_action(state, episode) # determine action through eps-greedy\n",
    "\n",
    "                new_observation, _, done, _ = self.env.step(\n",
    "                    action.item())                                  # get new state from decided action (reward & info -> blank)\n",
    "\n",
    "                if done:                                                # if pole lean down || iteration > max_episode_steps \n",
    "                    new_state = None                                    # No new state\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))                # save succeeded steps by every 'episode'\n",
    "                    \n",
    "                    if step < 495:                                          # if pole lean down( < max_episode_steps)\n",
    "                        reward = torch.FloatTensor([-1.0])                  # reward -> -1\n",
    "                        complete_episodes = 0                               # reset complete_episode\n",
    "                    else:                                                   # if pole does not lean down\n",
    "                        reward = torch.FloatTensor([1.0])                   # reward -> 1\n",
    "                        complete_episodes += 1                              # update succeeded number\n",
    "                else:                                                   # not done(still in interation)\n",
    "                    reward = torch.FloatTensor([0.0])                   # reward = 0\n",
    "                    new_state = new_observation                         # update new state\n",
    "                    new_state = torch.from_numpy(new_state).type(           # change type to FloatTensor\n",
    "                        torch.FloatTensor)\n",
    "                    new_state = torch.unsqueeze(new_state, 0)           # size 4 -> size 1*4 convert\n",
    "\n",
    "                self.network.mem.push(\n",
    "                    state, action, new_state, reward)               # store iteration information(Transition) into memory\n",
    "                self.network.replay()                               # update Q with Experience Replay\n",
    "                state = new_state                                   # update state\n",
    "\n",
    "                if done:\n",
    "                    print(f'{episode} Episode: Finished after {step + 1} steps：최근 10 에피소드의 평균 단계 수 = {episode_10_list.mean()}')\n",
    "                    break\n",
    "\n",
    "            if is_episode_final is True:\n",
    "                display_frames_as_gif(frames)                   # save result as gif\n",
    "                break\n",
    "\n",
    "            if complete_episodes >= 10:                         # (iteration > max_episode_steps) && (pole does not lean down) for 10 episodes\n",
    "                print('10 에피소드 연속 성공')\n",
    "                is_episode_final = True                        # save result and break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "균 단계 수 = 9.2\n680 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n681 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n682 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n683 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n684 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n685 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n686 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n687 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n688 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n689 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 8.9\n690 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 8.7\n691 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 8.7\n692 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 8.7\n693 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 8.9\n694 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n695 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n696 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n697 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n698 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n699 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n700 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n701 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n702 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n703 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n704 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n705 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n706 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n707 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n708 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n709 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n710 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n711 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n712 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n713 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n714 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n715 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n716 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n717 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n718 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n719 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n720 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n721 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n722 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n723 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n724 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n725 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n726 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n727 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n728 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n729 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n730 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n731 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n732 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n733 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n734 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n735 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n736 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n737 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n738 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n739 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n740 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n741 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n742 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n743 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n744 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n745 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n746 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n747 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n748 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n749 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n750 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n751 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n752 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n753 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n754 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n755 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n756 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n757 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n758 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n759 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n760 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n761 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n762 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n763 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n764 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.9\n765 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n766 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n767 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n768 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n769 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n770 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n771 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n772 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n773 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n774 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n775 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n776 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n777 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n778 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n779 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n780 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n781 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n782 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n783 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n784 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n785 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n786 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n787 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n788 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n789 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n790 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n791 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n792 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n793 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n794 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n795 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n796 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n797 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n798 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n799 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n800 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n801 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n802 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n803 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n804 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n805 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n806 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n807 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n808 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n809 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n810 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n811 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n812 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n813 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n814 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n815 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n816 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n817 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n818 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n819 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n820 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n821 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n822 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n823 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n824 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n825 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n826 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n827 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n828 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n829 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n830 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n831 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n832 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n833 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n834 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n835 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n836 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n837 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n838 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n839 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n840 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n841 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n842 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n843 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n844 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n845 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n846 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n847 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n848 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n849 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n850 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n851 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n852 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n853 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n854 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n855 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n856 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n857 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n858 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n859 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n860 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n861 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n862 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n863 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n864 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n865 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n866 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n867 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n868 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n869 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n870 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n871 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n872 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n873 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n874 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n875 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n876 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n877 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n878 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n879 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n880 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n881 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n882 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n883 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n884 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n885 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n886 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 8.8\n887 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 8.7\n888 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 8.7\n889 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 8.8\n890 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 8.8\n891 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 8.9\n892 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 8.9\n893 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 8.9\n894 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n895 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n896 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n897 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n898 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n899 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n900 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n901 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n902 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n903 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n904 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n905 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n906 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n907 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n908 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n909 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n910 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n911 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n912 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n913 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n914 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n915 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n916 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n917 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n918 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n919 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n920 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n921 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n922 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n923 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n924 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n925 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n926 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n927 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n928 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n929 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n930 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n931 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n932 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n933 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n934 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n935 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n936 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n937 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n938 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n939 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n940 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n941 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n942 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n943 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n944 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n945 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n946 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n947 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n948 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n949 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n950 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n951 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n952 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n953 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n954 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n955 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n956 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n957 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n958 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n959 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n960 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n961 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n962 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n963 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n964 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n965 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n966 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n967 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n968 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n969 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n970 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.7\n971 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.8\n972 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n973 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n974 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n975 Episode: Finished after 11 steps：최근 10 에피소드의 평균 단계 수 = 9.6\n976 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n977 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n978 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n979 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n980 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n981 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n982 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.5\n983 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n984 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n985 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n986 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n987 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n988 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n989 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.4\n990 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n991 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n992 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n993 Episode: Finished after 10 steps：최근 10 에피소드의 평균 단계 수 = 9.3\n994 Episode: Finished after 8 steps：최근 10 에피소드의 평균 단계 수 = 9.2\n995 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n996 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.1\n997 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 9.0\n998 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 8.9\n999 Episode: Finished after 9 steps：최근 10 에피소드의 평균 단계 수 = 8.8\n"
    }
   ],
   "source": [
    "cartpole_env = Environment()\n",
    "cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('pyt': conda)",
   "language": "python",
   "name": "python37864bitpytcondafd1b43a097a64d34bd7b82617eda6abc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}