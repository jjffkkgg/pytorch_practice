{"cells":[{"metadata":{},"cell_type":"markdown","source":"## To develope from tutorial A2C\n- Enable GPU calc\n- Another model net (ex) CNN\n- print & save result\n- export net and import"},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"colab_type":"code","executionInfo":{"elapsed":1130,"status":"error","timestamp":1599567495461,"user":{"displayName":"Taehwan Seo","photoUrl":"","userId":"06395709366113538068"},"user_tz":-540},"id":"ReU7Cds5c_NS","outputId":"e28e84e1-0315-4938-c9ca-3ab754e0bcda","trusted":true},"cell_type":"code","source":"from __future__ import annotations\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gym\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy","execution_count":1,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"vR1Z3B23eJZR","trusted":true},"cell_type":"code","source":"# set env with setting (especially reward and max episode)\ngym.envs.register(\n    id='CartPole_prefer-v0',\n    entry_point='gym.envs.classic_control:CartPoleEnv',\n    max_episode_steps=700,      # CartPole-v0 uses 200\n    reward_threshold=-110.0,\n)","execution_count":2,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"YjKI9y2ed3TF","trusted":true},"cell_type":"code","source":"'''Global Variables'''\nENV = 'CartPole_prefer-v0'  # 태스크 이름\nGAMMA = 0.99                # 시간할인율\nMAX_STEPS = 700             # 1에피소드 당 최대 단계 수\nNUM_EPISODES = 2000         # 최대 에피소드 수\n\nNUM_PROCESSES = 32          # 동시 실행 환경 수\nNUM_ADVANCED_STEP = 5       # 총 보상을 계산할 때 Advantage 학습(action actor)을 할 단계 수\n\nVALUE_LOSS_COEFF = 0.5\nENTROPY_COEFF = 0.01        # Local min 에서 벗어나기 위한 엔트로피 상수\nMAX_GRAD_NORM = 0.5\n\n# GPU를 사용할 경우\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":9,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Gx3uSUlDemU3","trusted":true},"cell_type":"code","source":"class RolloutStorage(object):\n    '''Advantage 학습에 사용할 메모리 클래스'''\n    def __init__(self, num_steps: int, num_processes: int, obs_shape: int) -> None:\n        '''initialize tensors for work'''\n        self.observations = torch.zeros(num_steps + 1, num_processes, obs_shape)        # obs tensor init\n        self.masks = torch.ones(num_steps + 1, num_processes, 1)                        # mask -> is end of episode ? 0 | 1\n        self.rewards = torch.zeros(num_steps, num_processes, 1)\n        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n\n        # 할인 총 보상(J(theta,s_t))저장하는 메모리 (Actor)\n        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n        self.index = 0              # insert 하는 index          \n\n    def insert(self, current_obs: tensor, action: tensor, reward: tensor, mask: FloatTensor) -> None:\n        '''현재 인덱스 위치에 transition을 저장'''\n        self.observations[self.index + 1].copy_(current_obs)\n        self.masks[self.index + 1].copy_(mask)\n        self.rewards[self.index].copy_(reward)\n        self.actions[self.index].copy_(action)\n\n        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 인덱스 값 업데이트\n\n    def after_update(self):\n        '''Advantage학습 단계만큼 단계가 진행되면 가장 새로운 transition을 index0에 저장'''\n        self.observations[0].copy_(self.observations[-1])\n        self.masks[0].copy_(self.masks[-1])\n\n    def compute_returns(self, next_value):\n        '''Advantage학습 범위 안의 각 단계에 대해 할인 총보상(J)을 계산'''\n\n        # 주의 : 5번째 단계부터 거슬러 올라오며 계산\n        # 주의 : 5번째 단계가 Advantage1, 4번째 단계는 Advantage2가 됨\n        self.returns[-1] = next_value\n        for ad_step in reversed(range(self.rewards.size(0))):\n            # advantage 고려 Q함수의 수정\n            self.returns[ad_step] = self.returns[ad_step + 1] * GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]","execution_count":4,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"colab_type":"code","executionInfo":{"elapsed":1928,"status":"error","timestamp":1599567608082,"user":{"displayName":"Taehwan Seo","photoUrl":"","userId":"06395709366113538068"},"user_tz":-540},"id":"J_dMPRXZem3w","outputId":"866bb726-43ca-4217-b65d-5c121a52aca5","trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, n_in: int, n_mid: int, n_out: int) -> None:\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(n_in,n_mid)\n        self.fc2 = nn.Linear(n_mid,n_mid)\n        self.actor = nn.Linear(n_mid, n_out)        # Actor net(two action output)\n        self.critic = nn.Linear(n_mid, 1)           # critic net (state value -> 1)\n    \n    def forward(self, x) -> list:\n        '''Forward wave 계산을 정의'''\n        h1 = F.relu(self.fc1(x))\n        h2 = F.relu(self.fc2(h1))\n        critic_output = self.critic(h2)\n        actor_output = self.actor(h2)\n\n        return critic_output, actor_output\n    \n    def act(self, x) -> Tensor:\n        '''상태 x로부터 행동을 확률적으로 결정'''\n        value, actor_output = self(x)\n        action_probs = F.softmax(actor_output, dim=1)       # softmax 를 통한 행동의 확률 계산\n        action = action_probs.multinomial(num_samples=1)\n        return action\n\n    def get_value(self, x):\n        '''상태 x로부터 상태가치를 계산'''\n        value, actor_output = self(x)\n\n        return value\n    \n    def evaluate_actions(self, x: tensor, actions) -> list:\n        '''상태 x로부터 상태가치, 실제 행동 actions의 로그 확률, 엔트로피를 계산'''\n        value, actor_output = self(x)\n\n        log_probs = F.log_softmax(actor_output, dim=1)      # dim=1이므로 행동의 종류에 대해 확률을 계산\n        action_log_probs = log_probs.gather(1, actions)    # 실제 행동의 로그 확률(log_probs)을 구함\n\n        probs = F.softmax(actor_output, dim=1)\n        entropy = -(log_probs * probs).sum(-1).mean()\n\n        return value, action_log_probs, entropy","execution_count":5,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"zx1k8jyneoyx","trusted":true},"cell_type":"code","source":"class Brain(object):\n    def __init__(self, actor_critic: Net) -> None:\n        self.actor_critic = actor_critic\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)    # learning rate = 0.01 (faster)\n        \n    def update(self, rollouts: RolloutStorage) -> None:\n        ''''Advantage학습의 대상이 되는 5단계 모두를 사용하여 수정'''\n        obs_shape = rollouts.observations.size()[2:]    # torch.Size([4, 84, 84])\n        num_steps = NUM_ADVANCED_STEP\n        num_processes = NUM_PROCESSES\n\n        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n            rollouts.observations[:-1].view(-1,4),\n            rollouts.actions.view(-1,1)\n        )\n\n        # 주의 : 각 변수의 크기\n        # rollouts.observations[:-1].view(-1, 4) torch.Size([80, 4])\n        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n        # values torch.Size([80, 1])\n        # action_log_probs torch.Size([80, 1])\n        # entropy torch.Size([])\n\n        values = values.view(num_steps, num_processes, 1)   # torch.Size([5, 16, 1])\n        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n\n        # advantage(행동가치-상태가치) 계산\n        advantages = rollouts.returns[:-1] - values          # torch.Size([5, 16, 1])\n\n        # Critic loss 계산\n        value_loss = advantages.pow(2).mean()\n\n        # Actor의 gain 계산, 나중에 -1을 곱하면 loss가 된다\n        action_gain = (action_log_probs * advantages.detach()).mean()\n        # detach 메서드를 호출하여 advantages를 상수로 취급\n\n        # 오차함수의 총합\n        total_loss = (value_loss * VALUE_LOSS_COEFF - action_gain - entropy * ENTROPY_COEFF)\n        \n        # 가중치 수정\n        self.actor_critic.train()\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        # 결합 가중치가 한번에 너무 크게 변화하지 않도록, 경사를 0.5 이하로 제한함(클리핑)\n        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), MAX_GRAD_NORM)\n\n        self.optimizer.step()","execution_count":6,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"Pl6ubIxseq-Y","trusted":true},"cell_type":"code","source":"class Environment:\n    def run(self) -> None:\n        '''running entry point'''\n        # 동시 실행할 환경 수 만큼 env를 생성\n        envs = [gym.make(ENV) for i in range(NUM_PROCESSES)]\n\n        # 모든 에이전트가 공유하는 Brain 객체를 생성\n        n_in = envs[0].observation_space.shape[0]           # state inputs\n        n_out = envs[0].action_space.n                      # action outpus\n        n_mid = 32                                          # 32 mid junction\n        actor_critic = Net(n_in, n_mid, n_out).to(DEVICE)   # Net init\n        glob_brain = Brain(actor_critic)                    # Brain init\n\n        # 각종 정보를 저장하는 변수\n        obs_shape = n_in\n        current_obs = torch.zeros(NUM_PROCESSES, obs_shape)                         # (16,4) 의 tensor\n        rollouts = RolloutStorage(NUM_ADVANCED_STEP, NUM_PROCESSES,obs_shape)       # RolloutStorage init\n        episode_rewards = torch.zeros(NUM_PROCESSES, 1)                             # 현재 episode 의 reward\n        final_rewards = torch.zeros(NUM_PROCESSES, 1)                               # 마지막 episode 의 reward\n        obs_np = np.zeros([NUM_PROCESSES, obs_shape])                               # state 배열\n        reward_np = np.zeros([NUM_PROCESSES, 1])                                    # 보상의 배열\n        done_np = np.zeros([NUM_PROCESSES, 1])                                      # Done 여부의 배열\n        each_step = np.zeros(NUM_PROCESSES)                                         # 각 env 의 step record\n        episode = 0\n        # 초기 state...\n        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n        obs = np.array(obs)\n        obs = torch.from_numpy(obs).float()                     # (16,4) 의 tensor\n        current_obs = obs                                       # current obs 의 업데이트\n\n        # advanced 학습(action actor)에 사용되는 객체 rollouts 첫번째 상태에 현재 상태를 저장\n        rollouts.observations[0].copy_(current_obs)\n\n        # 에피소드 반복문\n        for episode in range(NUM_PROCESSES * NUM_EPISODES):\n            # advanced 학습(action actor) 대상이 되는 각 단계에 대해 계산 (step 반복)\n            for step in range(NUM_ADVANCED_STEP):\n                # action 을 fetch\n                with torch.no_grad():\n                    action = actor_critic.act(rollouts.observations[step])\n                # (16,1)→(16,) -> tensor를 NumPy변수로\n                actions = action.squeeze(1).numpy()\n\n                # process 반복\n                for i in range(NUM_PROCESSES):\n                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(actions[i])\n\n                    # episode의 종료가치, state_next를 설정\n                    if done_np[i]:          # 지정된 step 달성 혹은 무너짐\n                        if i == 0:          # 0번째의 환경 결과만 출력\n                            print(f'{episode} Episode: Finished after {each_step[i] + 1} steps')\n                        episode += 1\n                        # 보상 부여\n                        if each_step[i] < (MAX_STEPS - 5):\n                            reward_np[i] = -1.0     # 무너졌을때\n                        else:\n                            reward_np[i] = 1.0      # 성공했을때\n                        \n                        each_step[i] = 0            # step 초기화\n                        obs_np[i] = envs[i].reset() # 환경 초기화\n                    else:                           # 무너지거나 성공도 아님\n                        reward_np[i] = 0.0\n                        each_step[i] += 1           # 그대로 진행\n\n                # 보상을 tensor로 변환하고, 에피소드의 총보상에 더해줌\n                reward = torch.from_numpy(reward_np).float()\n                episode_rewards += reward\n\n                # 각 실행 환경을 확인하여 done이 true이면 mask를 0으로, false이면 mask를 1로\n                masks = torch.FloatTensor([[0.0] if done_i else [1.0] for done_i in done_np])\n\n                # 마지막 에피소드의 총 보상을 업데이트\n                final_rewards *= masks      # done이 false이면 1을 곱하고, true이면 0을 곱해 초기화\n                # done이 false이면 0을 더하고, true이면 episode_rewards를 더해줌\n                final_rewards += (1 - masks)*episode_rewards\n\n                # 에피소드의 총보상을 업데이트\n                episode_rewards *= masks\n\n                # 현재 done이 true이면 모두 0으로 \n                current_obs *= masks\n\n                # current_obs를 업데이트\n                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\n                current_obs = obs  # 최신 상태의 obs를 저장\n\n                # 메모리 객체에 현 단계의 transition을 저장\n                rollouts.insert(current_obs, action.data, reward, masks)\n\n            # advanced 학습 for문 끝\n\n            # advanced 학습 대상 중 마지막 단계의 상태로 예측하는 상태가치를 계산\n\n            with torch.no_grad():\n                next_value = actor_critic.get_value(rollouts.observations[-1]).detach()\n                # next_value는 다음 state의 상태가치\n                # rollouts.observations의 크기는 torch.Size([6, 16, 4])\n            \n            # 모든 단계의 할인총보상을 계산하고, rollouts의 변수 returns를 업데이트\n            rollouts.compute_returns(next_value)\n\n            # 신경망 및 rollout 업데이트\n            glob_brain.update(rollouts)\n            rollouts.after_update()\n\n            # 모든 환경이 성공\n            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n                print('모든 환경 성공')\n                torch.save(actor_critic.state_dict(), \"./\")\n                break","execution_count":7,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"o0nn2cr_exWL","trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n    print(f'GPU 사용 가능 여부: {torch.cuda.is_available()}')\n    cartpole_env = Environment()\n    cartpole_env.run()","execution_count":11,"outputs":[{"output_type":"stream","text":"GPU 사용 가능 여부: False\n9 Episode: Finished after 15.0 steps\n9 Episode: Finished after 15.0 steps\n11 Episode: Finished after 26.0 steps\n13 Episode: Finished after 10.0 steps\n22 Episode: Finished after 9.0 steps\n23 Episode: Finished after 15.0 steps\n27 Episode: Finished after 24.0 steps\n33 Episode: Finished after 16.0 steps\n36 Episode: Finished after 20.0 steps\n46 Episode: Finished after 35.0 steps\n42 Episode: Finished after 18.0 steps\n48 Episode: Finished after 12.0 steps\n50 Episode: Finished after 36.0 steps\n55 Episode: Finished after 21.0 steps\n61 Episode: Finished after 25.0 steps\n68 Episode: Finished after 23.0 steps\n68 Episode: Finished after 12.0 steps\n75 Episode: Finished after 27.0 steps\n79 Episode: Finished after 16.0 steps\n78 Episode: Finished after 16.0 steps\n90 Episode: Finished after 61.0 steps\n99 Episode: Finished after 17.0 steps\n122 Episode: Finished after 140.0 steps\n126 Episode: Finished after 22.0 steps\n133 Episode: Finished after 28.0 steps\n138 Episode: Finished after 34.0 steps\n172 Episode: Finished after 165.0 steps\n197 Episode: Finished after 132.0 steps\n226 Episode: Finished after 143.0 steps\n258 Episode: Finished after 159.0 steps\n315 Episode: Finished after 282.0 steps\n330 Episode: Finished after 66.0 steps\n343 Episode: Finished after 69.0 steps\n370 Episode: Finished after 141.0 steps\n400 Episode: Finished after 151.0 steps\n438 Episode: Finished after 188.0 steps\n463 Episode: Finished after 127.0 steps\n485 Episode: Finished after 106.0 steps\n505 Episode: Finished after 100.0 steps\n571 Episode: Finished after 312.0 steps\n570 Episode: Finished after 17.0 steps\n580 Episode: Finished after 50.0 steps\n597 Episode: Finished after 81.0 steps\n602 Episode: Finished after 29.0 steps\n612 Episode: Finished after 52.0 steps\n620 Episode: Finished after 34.0 steps\n627 Episode: Finished after 23.0 steps\n632 Episode: Finished after 37.0 steps\n641 Episode: Finished after 49.0 steps\n659 Episode: Finished after 90.0 steps\n687 Episode: Finished after 128.0 steps\n709 Episode: Finished after 123.0 steps\n723 Episode: Finished after 71.0 steps\n735 Episode: Finished after 47.0 steps\n747 Episode: Finished after 71.0 steps\n767 Episode: Finished after 83.0 steps\n788 Episode: Finished after 114.0 steps\n804 Episode: Finished after 66.0 steps\n809 Episode: Finished after 10.0 steps\n811 Episode: Finished after 9.0 steps\n814 Episode: Finished after 10.0 steps\n814 Episode: Finished after 10.0 steps\n820 Episode: Finished after 12.0 steps\n812 Episode: Finished after 11.0 steps\n831 Episode: Finished after 78.0 steps\n864 Episode: Finished after 181.0 steps\n911 Episode: Finished after 238.0 steps\n935 Episode: Finished after 107.0 steps\n948 Episode: Finished after 68.0 steps\n955 Episode: Finished after 22.0 steps\n966 Episode: Finished after 44.0 steps\n965 Episode: Finished after 15.0 steps\n967 Episode: Finished after 22.0 steps\n970 Episode: Finished after 15.0 steps\n976 Episode: Finished after 13.0 steps\n978 Episode: Finished after 27.0 steps\n981 Episode: Finished after 15.0 steps\n986 Episode: Finished after 25.0 steps\n994 Episode: Finished after 28.0 steps\n995 Episode: Finished after 17.0 steps\n1002 Episode: Finished after 28.0 steps\n1006 Episode: Finished after 19.0 steps\n1014 Episode: Finished after 17.0 steps\n1016 Episode: Finished after 32.0 steps\n1023 Episode: Finished after 37.0 steps\n1047 Episode: Finished after 128.0 steps\n1092 Episode: Finished after 225.0 steps\n1125 Episode: Finished after 165.0 steps\n1172 Episode: Finished after 235.0 steps\n1185 Episode: Finished after 51.0 steps\n1324 Episode: Finished after 699.0 steps\n1337 Episode: Finished after 74.0 steps\n1354 Episode: Finished after 49.0 steps\n1352 Episode: Finished after 13.0 steps\n1352 Episode: Finished after 15.0 steps\n1407 Episode: Finished after 273.0 steps\n1412 Episode: Finished after 26.0 steps\n1511 Episode: Finished after 495.0 steps\n1525 Episode: Finished after 71.0 steps\n1643 Episode: Finished after 585.0 steps\n1782 Episode: Finished after 700.0 steps\n1807 Episode: Finished after 122.0 steps\n1811 Episode: Finished after 25.0 steps\n1932 Episode: Finished after 604.0 steps\n2013 Episode: Finished after 406.0 steps\n2153 Episode: Finished after 700.0 steps\n2293 Episode: Finished after 700.0 steps\n2312 Episode: Finished after 94.0 steps\n2328 Episode: Finished after 60.0 steps\n2343 Episode: Finished after 76.0 steps\n2352 Episode: Finished after 61.0 steps\n2366 Episode: Finished after 70.0 steps\n2380 Episode: Finished after 70.0 steps\n2388 Episode: Finished after 41.0 steps\n2397 Episode: Finished after 43.0 steps\n2409 Episode: Finished after 48.0 steps\n2411 Episode: Finished after 23.0 steps\n2418 Episode: Finished after 33.0 steps\n2426 Episode: Finished after 42.0 steps\n2435 Episode: Finished after 32.0 steps\n2436 Episode: Finished after 18.0 steps\n2444 Episode: Finished after 19.0 steps\n2445 Episode: Finished after 28.0 steps\n2452 Episode: Finished after 37.0 steps\n2462 Episode: Finished after 44.0 steps\n2470 Episode: Finished after 42.0 steps\n2489 Episode: Finished after 97.0 steps\n2513 Episode: Finished after 122.0 steps\n2566 Episode: Finished after 259.0 steps\n2608 Episode: Finished after 214.0 steps\n2638 Episode: Finished after 148.0 steps\n2687 Episode: Finished after 245.0 steps\n2723 Episode: Finished after 180.0 steps\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-a6c483f92e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'GPU 사용 가능 여부: {torch.cuda.is_available()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcartpole_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcartpole_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-d1dcaa24f7d8>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# 신경망 및 rollout 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mglob_brain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-4106773f1808>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_GRAD_NORM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"inv_pend_A2C.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}